{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cec4646-80b2-4835-a377-ebc7ec084b2a",
   "metadata": {},
   "source": [
    "Welcome to DS 6600: Data Engineering in the UVA Data Science Ph.D. Program!\n",
    "\n",
    "Please see the course syllabus for all of the details about the course.\n",
    "\n",
    "We will be using this Canvas page only for the following tasks:\n",
    "\n",
    "1. You will submit your assignments through Canvas (the Assignments tab)\n",
    "\n",
    "2. You can use Canvas to see the recordings of our classes for your reference or to complete the live coding requirement for class. To view a recording, click on Online Meetings, then Cloud Recordings.\n",
    "\n",
    "3. We will post grades to Canvas via the Grades tab.\n",
    "\n",
    "4. We will post links to the recommended readings and important notes for each class to this front page.\n",
    "\n",
    "\n",
    "Links to daily readings, notes, and resources\n",
    "\n",
    "Thursday, August 31 and Tuesday, Sept 5 (Intro to Docker)\n",
    "\n",
    "Tuesday, August 29 (pyenv and pipenv)\n",
    "\n",
    "Thursday, August 24 (Git and GitHub)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thursday, August 31 and Tuesday, Sept 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A very good introduction to the concept of containers: https://youtu.be/2YMu9bzDJbY?si=gRHrkWkLY3YrUEFZ\n",
    "\n",
    "Here's another wonderful tutorial, more hands-on than conceptual: https://youtu.be/3c-iBn73dDE?si=g49dEagljtnxAMo2\n",
    "\n",
    "Articles discussing the difference between virtual environments, containers, and virtual machines:\n",
    "https://stephen-odaibo.medium.com/docker-containers-python-virtual-environments-virtual-machines-d00aa9b8475\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-981-13-1501-5_12\n",
    "\n",
    "Docker tutorials: https://docs.docker.com/language/python/\n",
    "\n",
    "Official Docker documentation: https://docs.docker.com/\n",
    "\n",
    "Understanding and Using Containers\n",
    "\n",
    "1. Four levels of isolation: the global environment, virtual environments, containers, and virtual machines\n",
    "\n",
    "One of the most important concepts in software development is the level of isolation of the system you use to develop the software and host an application. According to Wikipedia, isolation is \"a set of different hardware and software technologies designed to protect each process from other processes on the operating system. It does so by preventing process A from writing to process B.\" In other words, an isolated environment does not save any files or install any programs outside that environment.\n",
    "\n",
    "There are four approaches with regard to isolation, and as a data engineer you need to choose the appropriate approach from the outset of your project:\n",
    "\n",
    "The global environment, the default setup of your computer, doesn't isolate anything;\n",
    "a virtual environment isolates the version of Python and the packages installed on that version of Python from everything else on the computer;\n",
    "a container isolates Python and packages as well, but also isolates the computer's operating system (Mac, Windows, or Linux) and additional applications from everything else on the computer;\n",
    "and a virtual machine isolates everything.\n",
    "\n",
    "Here is more information about each approach:\n",
    "\n",
    "The global environment refers to the main computation and storage on your computer that every piece of software shares by default, whether that's your web browser, email client, video games, etc. Unless you take explicit steps to use a virtual environment, container, or virtual machine, you are running your code and downloading packages in the global environment. The global environment is the least isolated option. When you develop software in this location, all kinds of crazy things can happen as your code can accidentally interact with other packages and with other applications installed on your computer. In addition, any changes you make to the system for a project in the global environment can change and potentially break everything else on your computer: for example, if a project requires you to use an older version of Python, changing it here changes it for all the packages you've ever installed to the global environment, and some of those packages might not work anymore.\n",
    "\n",
    "Pros: When everyone first learns Python or any programming language, they work in the global environment. It is straightforward and takes the least amount of time to use.  \n",
    "Cons: It is hard to tell how your code will interact with everything else installed on your computer. The way that usually plays out is that you hit some sort of error as you code, so you investigate and debug as you go, but while the adaptations you make resolve the specific errors you encountered, they also make your code much too specific to your particular computer and it won't run anymore on another system. That's a big problem if you are trying to share code that runs and works as intended on someone else's computer. It also means that the software you write in the global environment depends on the versions of the packages you've installed. If you wrote the software using Python 3.8 and numpy 1.18, there's no guarantee the software still works on a system using Python 3.11 and numpy 1.25.\n",
    "\n",
    "A virtual environment (not to be confused with a virtual machine) begins with a new, empty folder on your computer (called the project folder). Once you register this folder as a virtual environment you can install any version of Python you want in this folder and you install the minimum number of packages with specific versions in this folder as well. If you are running Python 3.10 in your global environment, you can run Python 3.8 in this virtual environment. Some packages wreak havoc with other packages (the package for connecting to Google's Colab service also likes to downgrade several dozen other packages to versions at least two years old), and if you need to use a package like this it would be wise to use a virtual environment instead of a global one.\n",
    "\n",
    "Pros: Using a dedicated, empty folder for a project helps keep everything organized. Virtual environments allow for easy control of the versions of Python and packages, and allow you to choose the minimum number of packages to install to keep the environment \"clean\". Creating a virtual environment with pipenv also creates a Pipfile that documents the specifications of the environment in a straightforward way.\n",
    "Cons: It takes a good deal of technological sophistication to move from using the global environment to using virtual environments or any other isolated system. That adds to Python's learning curve, which is already steep enough, so virtual environments are best taught to people who want to move quickly beyond the beginner stages. Also, while virtual environments allow for the control of Python and packages, they still operate under a particular computer's operating system. If you wrote software in a virtual environment on a Windows machine, there's no guarantee that it will work on a Mac, for example.\n",
    "\n",
    "A container is a virtual environment that adds the ability to change the operating system inside the environment and allows for the installation of other software external to Python, such as database management systems. You can use a container to run Windows on a Mac, or vice versa, or Linux on any system. The major client for building and managing containers is called Docker, and containers can be stored and shared for free via a website called Docker Hub (https://hub.docker.com).\n",
    "\n",
    "Pros: A docker container makes software very portable. A developer with a Windows computer, for example, can use a container to develop in Linux. Docker and Docker Hub make heavy software that is difficult and disruptive to install, like an operating system, worlds easier to use on any computer. PostgreSQL and other database management systems are notoriously difficult to install on the global environment but are relatively straightforward to use once you've gotten used to docker. Docker Hub is free to use and very popular, and most important software for data science is installed on a container on Docker Hub that you can install locally with minimal code, and you can upload and share your own containers here too.\n",
    "Cons: Docker can be confusing to learn and deploy correctly, and it is another system on top of Python and virtual environments to learn. In addition, depending on the software and files contained within a container, they can get very big very quickly, and big containers can take a long time to deploy on a computer. Some containers are too big to fit on to your computer at all. Finally, although containers use their own operating systems and software, they still depend on your computer's storage and computation hardware. A container can't run more quickly than your computer's processor will allow.\n",
    "\n",
    "A virtual machine (not to be confused with a virtual environment) is an entirely separate computer from your own. Typically (though not necessarily always) virtual machines (VMs) can be accessed through a cloud computing network like Amazon Web Services or Microsoft Azure and physically exist in a data center somewhere: in Charlottesville, most VMs someone will access through AWS are stored in Amazon's massive data center in Ashburn, Virginia. VMs often come pre-loaded with particular sets of software, and cloud compute companies charge you based on the amount of memory a VM uses and on the hardware connected to the VM. You can request a graphics processing unit (GPU) as a processor for your VM, which is several orders of magnitude faster than the standard central processing unit (CPU) you have on your laptop, but be prepared to pay through the nose for that privilege.\n",
    "\n",
    "Pros: A VM is similar to a container in that it allows you to fully control the Python version, the packages, and the operating system and software installed on the VM. Unlike a container, it also allows you to choose different physical resources for the system to increase the storage and computational speed of the system. That's crucial for big data applications in which the data are much too big to fit on your laptop, and a machine learning model will run far too slowly.\n",
    "Cons: You have to be extremely careful to protect the credentials you use to log in to the VM. There are so many scammers out there running web-scrapers to find files on public websites like GitHub where people saved their cloud compute keys. With these keys the scammer can install things like Bitcoin miners on your VM that drive up the memory consumption of the VM, sending the cryptocurrency to the scammer's wallet, and footing you with the bill. At any rate, cloud systems are yet another system to master on top of everything else, but unlike virtual environments and containers, these are not free and mistakes can end up costing a lot of money.\n",
    "\n",
    "2. When should you use a global environment, virtual environment, container, or virtual machine?\n",
    "\n",
    "The main considerations over a choice of environmental management are whether there are conflicts within your own global environment, whether or not you are able to install all the needed software and packages in your global environment, whether and with whom you need to share your code, and whether you need access to more computation and hard disk storage than you have access to on your own computer.\n",
    "\n",
    "Conflicts within your global environment occur when some packages only work on a version of Python you do not have, or if packages conflict with one another in a way that breaks a package you need for other projects. Many packages, especially the popular and well-maintained ones, try to stay current to avoid these sorts of conflicts. But sometimes the constant stream of updates leads to one package in the chain of dependencies breaking.\n",
    "\n",
    "Source: XKCD Comics #2347\n",
    "\n",
    "\n",
    "When this happens, you will usually see a cryptic error with the import command. Do some digging and you will usually see posts by other people discussing the conflict of versions, often sharing advice to downgrade some package to an earlier version. If you see issues like this, it is best to use a virtual environment or a container to manage the package versions for a particular project without forcing your global environment to have to get back with out of date versions of Python and of important packages.\n",
    "\n",
    "You might not be able to install all the software and packages you need in your global environment. Package repositories like CRAN for R require submissions to work on Mac, Windows, and Linux, but PyPI, the Python package repository, allows packages that only work with some or even one operating system. If you need a package but have the wrong operating system, you can use a container with the operating system you need for the package installed.\n",
    "\n",
    "There are many reasons why you might want to share your code. In academia and science, it is important to make your work reproducible. Reproducibility means allowing someone else to exactly replicate your findings by running the same code you used on the same data. If there is a question of whether the code would produce the same results using different versions of Python and packages, then it is important to supply a Pipfile for a virtual environment along with the code and data. If there is a concern that the code would either not work or generate different results on a different operating system as well, then provide a container with the code, data, and appropriate software included. You might consider a container in either case because that would allow you to use Docker Hub to store the container online, which makes it much easier to share.\n",
    "\n",
    "If you are working with big data, you probably need more resources for computation and storage than you have access to on your own computer. In this case you will need to use a virtual machine on a cloud computing service. You can rigidly control the VM's environment, and you can share the VM by either sharing access keys (which you should only do with your direct collaborators) or by creating a container image from the VM and sharing it via Docker Hub or GitHub. But if you don't need access to the physical resources available on the cloud, spinning up a VM might be overkill as it creates more work to manage access and it can cost a significant amount of money. There is another common use of VMs: VMs can be connected to a public IP address and hosted on the internet. If you are writing software such as a dashboard that you want to make accessible on the internet through a URL, using a VM is a good approach.\n",
    "\n",
    "3. requirements.txt files\n",
    "\n",
    "Before we discuss containers, it is helpful to define a requirements.txt file because we will use this file to define and launch a container.\n",
    "\n",
    "A requirements.txt file is a plain text file with every package installed (or to be installed) in a particular environment, along with the version numbers of the packages. Every package is listed on a new line. To see the requirements.txt list of your global environment, open a terminal window and type pip freeze. You can save this list in a text file directly by typing pip freeze > requirements.txt. You can also easily go from a Pipfile created by setting a virtual environment with pipenv to a requirements.txt file by copy-and-pasting the packages listed under [packages] and [dev-packages] into a new text file and saving the file as requirements.txt.\n",
    "\n",
    "If you have a requirements.txt file, but you do not yet have any of the packages listed there installed, you can install all of them at once by typing pip install -r requirements.txt, where -r tells pip to read from the file you provided. This is a good step to take when you update your global Python installation and want to bring all of your packages forward into the new Python version.\n",
    "\n",
    "Let's create a simple requirements.txt file. First create a new, empty folder for this project. Then open a new file in this folder using any plain text editor you feel most comfortable using. You can use JupyterLab by clicking File -> New -> Text File. Before anything else, save the file as requirements.txt. Let's put the following versions of JupyterLab (so we can use JupyterLab within the Docker container), numpy, and pandas in the file:\n",
    "\n",
    "jupyterlab==3.4.5\n",
    "\n",
    "numpy==1.23.2\n",
    "\n",
    "pandas==1.4.3\n",
    "\n",
    "Then save the file.\n",
    "\n",
    "4. Terminology of Docker\n",
    "\n",
    "Every new system we learn has its own set of terminologies to learn. With Docker, the most important words to learn are Dockerfile, Docker image, Docker container, and Docker compose.\n",
    "\n",
    "A Dockerfile is a plain text file with a lightweight programming syntax that provides instructions for what the container will eventually have installed. In the Dockerfile we can specify the version of Python, the operating system, whether additional software should be installed, and we can provide additional commands such as pip install -r requirements.txt to install all the Python packages we need from a requirements.txt file.\n",
    "\n",
    "A Docker image is a collection of all the files that are needed to create the container. It is similar to a zipped directory or to an installed Python package in that all the necessary files are present, but in its current state it does nothing until it is unzipped, imported, or activated. A Docker image gets built when we process a Dockerfile. If we say in the Dockerfile that we want an installation of Linux with Python 3.9 installed, PostgreSQL, and pandas, numpy, and matplotlib, then the Docker image reads this file and downloads all of these software packages. The image waits for another command to extract all these software to construct the container. Building the Docker image can take a while depending on the size of the software packages we instruct it to install.\n",
    "\n",
    "A Docker container is the deployment of a Docker image. When we issue the command to run a Docker image, Docker allocates space on our computer and creates a virtual environment in that space, then it runs all of the software in that virtual environment. Like virtual environments, containers can be accessed either interactively or in the background.\n",
    "\n",
    "A Docker compose file is a text file with another lightweight coding language that can be used to manage multiple Docker containers. It is important for organizing different containers and efficiently allocating enough space on the system to run all of them. We will need to use Docker compose for running multiple databases, for example, where one Docker container runs a relational database, one container runs a document database, and one runs a graph database.\n",
    "\n",
    "5. Docker Hub\n",
    "\n",
    "Docker Hub is a web-based repository of Docker images. It has two primary functions:\n",
    "\n",
    "It provides free space to any registered user to upload their own Docker images. That allows someone to define a set of containers that are important for their work and to have a fixed and external location for the images of those containers. It also allows someone to easily share a Docker image with someone else by uploading the image to Docker Hub and sending the sharing URL link.\n",
    "It contains a massive repository of Docker images containing different software and specifications that are free and accessible to anyone. You can search through these images here: https://hub.docker.com/search?q= Once you are comfortable with Docker and Docker Hub, downloading images containing the software you need from Docker Hub is probably the easiest way to get the software. That's especially true for complicated software such as database management systems.\n",
    "\n",
    "Take a few minutes to register for a Docker Hub account and to search around the images to see what is available.\n",
    "\n",
    "6. Writing a Dockerfile\n",
    "\n",
    "The first step in creating a container is to write a Dockerfile. First, make sure you have Docker Desktop installed in the global environment of your local machine.\n",
    "\n",
    "As a Dockerfile is a plain text file, start by opening a new file in any plain text editor you feel most comfortable using. Before anything else, save the file as Dockerfile. Make sure you capitalize the D but not the f, and make sure there is no file extension (so make sure it isn't accidentally saved as Dockerfile.txt). In JupyterLab click File -> Save Text As, then type Dockerfile in the window.\n",
    "\n",
    "On the top line of the text file, type\n",
    "\n",
    "# syntax=docker/dockerfile:1\n",
    "\n",
    "This tells the Docker image compiler to use the latest version of the Dockerfile syntax, and not to use either an older version nor an experimental version. This line is one that will probably appear at the top of every Dockerfile you write without you having to change it or think too much about it.\n",
    "\n",
    "Next we will take an image from Docker Hub as a starting point for the image we want to build. First, find an image on Docker Hub that installs the software you need. For this example, let's use the python:3.8-slim-buster image, which installs Linux and Python 3.8. All you need to do to install this image is to write the following line in your Dockerfile:\n",
    "\n",
    "FROM python:3.8-slim-buster\n",
    "\n",
    "We can now add to this image with additional lines of code. First, remember that the main concept behind a container is isolating the container from the rest of the computer. That means that the files in our project folder are not automatically included in the container. Let's first copy the requirements.txt file into the container by typing\n",
    "\n",
    "COPY requirements.txt requirements.txt\n",
    "\n",
    "The COPY command tells Docker to create a file in the container named requirements.txt, and to create that file by copying the file in the project folder named requirements.txt. The first occurrence of this name refers to what we want to name the file inside the container, and the second occurrence of the name refers to what the file is actually called in the project folder.\n",
    "\n",
    "Next, let's install the packages in our requirements.txt file by adding this line to the Dockerfile:\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "Dockerfiles can also be used in the same way as a .env file to define environmental variables with sensitive data such as passwords and API keys. Using environmental variables means that you don't have to type out your passwords and keys in the Python notebooks and scripts you write. To define an environmental variable named secretpassword, type:\n",
    "\n",
    "ENV secretpassword=whenyourehereyourefamily\n",
    "\n",
    "But be careful: if you use a Dockerfile to define environmental variables, then those variables will display in the Dockerfile and can be called and viewed in the container. Don't share your Docker image on Docker Hub if you've defined environmental variables in the Dockerfile. A better approach would be to share a Docker image with no environmental variables, then write a Dockerfile that you store locally that adds environmental variables to this image. (We'll talk more about how to do that later)\n",
    "\n",
    "Docker is especially useful for running applications that run continuously, such as dashboards. In this example, we will run JupyterLab as a continuously running application within the container. Running JupyterLab requires three lines of code within the Dockerfile.\n",
    "\n",
    "First, we need to define a folder that will exist inside the container that will store the files (such as .ipynb notebook files) that we will create using JupyterLab. We create this folder with this line:\n",
    "\n",
    "WORKDIR /notebooks\n",
    "\n",
    "Note that we could name the folder anything we want, but \"notebooks\" makes sense for us in this situation.\n",
    "\n",
    "Second, we need to define a port number inside the container for JupyterLab to run on. According to a website called CloudFlare: \"Ports allow computers to easily differentiate between different kinds of traffic: emails go to a different port than webpages, for instance, even though both reach a computer over the same Internet connection.\" JupyterLab, by default, runs on port 8888 on a computer. We need to open this port in the container so that JupyterLab can use it, and we do that with this command:\n",
    "\n",
    "EXPOSE 8888\n",
    "\n",
    "Finally, we need to specify a command for Docker to run when it launches the container. To launch JupyterLab from the command line in our global environment, we type JupyterLab. But within the container, we need to add two arguments: --ip=0.0.0.0 tells JupyterLab explicitly to run on the local host that exists within the container, and --allow-root allows us to run JupyterLab without having to define any specific user accounts.\n",
    "\n",
    "For some reason, Docker wants every word within the command to be passed as a string element within a list. So we have to type the command like this:\n",
    "\n",
    "CMD [\"jupyter\", \"lab\",\"--ip=0.0.0.0\",\"--allow-root\"]\n",
    "\n",
    "Then save the Dockerfile.\n",
    "\n",
    "There are many more Dockerfile commands we can and will use. But for introductory purposes let's leave it here and return to the construction of Dockerfiles later.\n",
    "\n",
    "7. Building a Docker image\n",
    "\n",
    "To build the Docker image from the Dockerfile we just wrote, open the terminal and type\n",
    "\n",
    "docker build . -t myfirstdocker\n",
    "\n",
    "docker build is the command that constructs an image from a Dockerfile. The . that appears third refers to the current project directory on your computer. By default Docker is looking for a file named Dockerfile, and it is best practice to always have one project folder for every container, and to have one file named exactly Dockerfile in each of these folders. The -t is a tag that tells Docker you want to provide a name to the container once it is running, and the last word in this command is the name we select.\n",
    "\n",
    "When you run this, look at what appears on the screen. Notice that Docker installs the image from Docker Hub, then installs Jupyter, numpy, and pandas, and then copies the files we instructed and runs (but does not yet display) the two lines we specified with RUN.\n",
    "\n",
    "Now the image is built, and the container is ready to launch.\n",
    "\n",
    "Before we do so, open the Docker Desktop App (see the little ship icon). Click on images. You should see an image named myfirstdocker.\n",
    "\n",
    "8. Activating the container\n",
    "\n",
    "Now that the image is built and named \"myfirstdocker\", we can deploy the image to start the container by typing:\n",
    "\n",
    "docker run -p 8888:8888 myfirstdocker\n",
    "\n",
    "Let's break down what this command does. docker run is the core command that reads an existing Docker image and attempts to launch it as a container. If this command can't find the image locally, it will look on Docker Hub for the image. The -p flag tells Docker that you are about to define a mapping from the port inside the container to a port on your computer. The first number is the container port and the second number is your computer's port. In this case, Docker will take the program that runs on port 8888 in the container and run it on port 8888 on your computer. These numbers don't have to be the same. If port 8888 is already being used (maybe by a local JupyterLab instance) you wanted JupyterLab to run on port 90 on your computer, for example, you can type -p 8888:90. Finally you provide the name of the image you want to use to create the container.\n",
    "\n",
    "Run this command, and your first Docker container is alive! JupyterLab displays the same text you see when you type JupyterLab into the command line. You will see a URL towards the bottom: copy this URL into a web browser and it will take you to the JupyterLab running inside the Docker container.\n",
    "\n",
    "A few things to notice:\n",
    "\n",
    "Open a new notebook and save it. Type !python --version into the first code cell. This version will always be the same version of Python we defined in the FROM command in the Dockerfile.\n",
    "\n",
    "Try to import some packages. You can import pandas and numpy, but you can't import other packages like seaborn that you did not include in your requirements.txt file.\n",
    "\n",
    "Type\n",
    "\n",
    "import os\n",
    "\n",
    "password = os.environ['secretpassword']\n",
    "\n",
    "then type password alone in a code cell. You will see your password that you saved as an environmental variable display in the output. Now you can supply passwords and secret keys to anything that requires them in your code without having to actually type them out. Just don't publicly share your Dockerfile or Docker image if you use this method of defining environmental variables.\n",
    "\n",
    "Open a new terminal window. The command prompt looks different from what you are used to. That's because you are now running Linux. Mac users are running Linux. Windows users are running Linux. We've conquered all our problems with different hardware (for now)!\n",
    "\n",
    "9. Copying files from the container back to your global environment\n",
    "\n",
    "Without closing the terminal window in which JupyterLab is running, open a new terminal window and navigate to your container's project folder (the same place you saved your Dockerfile). If you type\n",
    "\n",
    "docker ps\n",
    "\n",
    "you will see a list of all your active containers that tells you the container ID (a unique combination of letters and numbers) and the (randomly generated) container name. You can use either the container ID or the container name to execute commands that refer to this container.\n",
    "\n",
    "In JupyterLab, save your notebook as dockernotebook.ipynb. Right now, this notebook only lives inside the container. If you close JupyterLab then the container will close and this file will be deleted.\n",
    "\n",
    "To copy this file to your container's project folder, type the following (just replace \"elastic_hertz\" with whatever random name your container received):\n",
    "\n",
    "docker cp elastic_hertz:/notebooks/dockernotebook.ipynb ./dockernotebook.ipynb\n",
    "\n",
    "Now the dockernotebook.ipynb file is saved in your project folder and will remain accessible in your global environment even when the container closes.\n",
    "\n",
    "10. Best practice for setting environmental variables if you plan to post your Dockerfile publicly to Docker Hub\n",
    "\n",
    "Although the Dockerfile's ENV command makes it pretty to store passwords and keys as environmental variables, it also creates a privacy and security risk if you want to store your Docker image on Docker Hub, or if you want to store your code on GitHub, because anyone who sees the Docker file or runs the image can see your environmental variables, which include sensitive passwords and keys. Some bad actors run web-scraping scripts on Docker Hub and GitHub to find and exploit exposed environmental variables.\n",
    "\n",
    "A better practice is to always save your environmental variables locally, on your own computer, and never on any public web-based repository. Here's how to accomplish that:\n",
    "\n",
    "First: create a new text file using any text editor you feel comfortable with, and save the file as .env. This file will contain all of your environmental variables, and naming it .env is a standard convention. Following this convention makes it easier for your code to work on other people's computers: if everyone's passwords are saved in a file named .env, then the code can always load from whatever .env file is present, even if the .env contains different passwords. Remember to make sure the file is named exactly .env and not accidentally .env.txt. Also remember that files that begin with a . generally are treated as hidden files on both Mac and Windows systems because these files are considered to be more essential or sensitive to the operation of a computer system and hiding them is good practice to discourage users from changing or deleting these files.\n",
    "\n",
    "Inside the .env file, type all the variables you want to save. Type different environmental variables on different lines. For example:\n",
    "\n",
    "secretpassword=mydndcharacterisanightelf\n",
    "\n",
    "secretapikey=123456789\n",
    "\n",
    "Then save the .env file.\n",
    "\n",
    "Second: Delete all ENV commands in your Dockerfile if any are present.\n",
    "\n",
    "Third: Use docker build to create the Docker image. Then use docker run to launch the container for this image, but add the following option just before the name of your container: --env-file=.env. For example:\n",
    "\n",
    "docker build . -t imagename\n",
    "\n",
    "docker run -p 8889:8888 --env-file=.env imagename\n",
    "\n",
    "The --env-file option loads all the environmental variables from your local .env file into the container, but it does not include the .env file or the environmental variables in the Dockerfile or in the Docker image. If someone else has your image, they can use the --env-file option of Docker run to load their own environmental variables from their own local .env file.\n",
    "\n",
    "Fourth: If you are using the container to run Python (or Python within JupyterLab), type the following Python code to use your local environmental variables:\n",
    "\n",
    "import os\n",
    "password = os.getenv('secretpassword')\n",
    "apikey = os.getenv('secretapikey')\n",
    "\n",
    "Now the password and apikey variables contain your credentials, and you can use these in your subsequent code.\n",
    "\n",
    "11. Uploading a built Docker image to Docker Hub\n",
    "\n",
    "Docker Hub provides free, online storage for Docker images. Once you've registered for an account on Docker Hub, start by logging into Docker Hub on the command line, which you can do by simply typing\n",
    "\n",
    "docker login\n",
    "\n",
    "You might be prompted to enter your Docker Hub username and password, or it might remember the credentials you supplied previously. Either way, you need to see the response message \"Login Succeeded\".\n",
    "\n",
    "The goal is to upload an image you've created locally to Docker Hub. The image you created has a name, and it will have a name on Docker Hub as well, but this name does not need to be the same as the local name. To set the image's name on Docker Hub, use the docker tag command, such as\n",
    "\n",
    "docker tag myfirstdocker jkropko/python3_8_jupyterlab\n",
    "\n",
    "Here, after typing docker tag we write the name of the local image, which was myfirstdocker in the example we worked through above. Then we write the name the container will have on Docker Hub. A couple points about how to name a container: first, the name must begin with your own account name, then a slash. Second, the name should be concise but also descriptive of the actual content of the container, which is why I wrote python3_8_jupyterlab instead of something vague like myfirstdocker.\n",
    "\n",
    "Finally to upload the image to Docker Hub, use the docker push command:\n",
    "\n",
    "docker push jkropko/python3_8_jupyterlab\n",
    "\n",
    "The system takes a few minutes to upload all of the files, but once it completes, you can see your repository by going to https://hub.docker.com and signing in. You should see your new image listed on the home page. If you click the image you just uploaded, you can edit the description and see the version history (if new versions have been uploaded) along with the download statistics.\n",
    "\n",
    "When you create an account on Docker Hub, the system creates a \"top level\" repository for you that has your username. I just saved my image to my top level repository. Also, the top level repository is by default public, which means anyone can access the image I just saved. You can load my image onto your computer by typing\n",
    "\n",
    "docker run -p 8888:8888 jkropko/python3_8_jupyterlab\n",
    "\n",
    "(Note, if you have a local instance of JupyterLab running, change the first 8888 to 8889 and also replace the 8888 to 8889 in the web address the container lists when you run it.)\n",
    "\n",
    "Sharing images via Docker Hub is an awesome and super-evolved way to share environments with collaborators, reviewers, or anyone else who might want to take a close look at your code and data.\n",
    "\n",
    "You might want to save some images on Docker Hub for your own use and access, but you might want these images to be private. Docker Hub allows you to create one private repository before charging you for a premium service, but allows you to save many images inside this one private repository. To create the private repository, go to the Docker Hub home page and click \"Create Repository\". Next to your user name, I recommend naming the repository \"private\" so that you can easily remember that this folder is for your private Docker images. Add a description if you want, and make sure private is selected. Then push \"Create\".\n",
    "\n",
    "Now that the private repository is created, you can tag and push local images to the private repository by adding private: before the name of the image on Docker Hub. For example, I create a private version of the same image I pushed publicly by typing:\n",
    "\n",
    "docker push jkropko/private:python3_8_jupyterlab\n",
    "\n",
    "docker push jkropko/private:python3_8_jupyterlab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tuesday, August 29\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Installation and usage instructions for pyenv: https://github.com/pyenv/pyenv\n",
    "\n",
    "A great video introduction to pipenv: https://youtu.be/zDYL22QNiWk\n",
    "\n",
    "pipenv official documentation: https://pipenv.pypa.io/en/latest/\n",
    "\n",
    "A good general discussion of virtual environments in Python: https://realpython.com/python-virtual-environments-a-primer/#why-do-you-need-virtual-environments\n",
    "\n",
    "Notes on installation and use of pyenv/pipenv:\n",
    "\n",
    "What exactly is an environment?\n",
    "\n",
    "\"The environment\" means all of the software and data stored on your computer. When you launch software, there is a specific version of that software. It may call other background programs or data that are also stored on your computer. All of this together forms your computer's environment.\n",
    "\n",
    "What is a virtual environment?\n",
    "\n",
    "A virtual environment is isolated space on your computer on which different software/data, or different versions of already installed software/data can be installed. When inside the virtual environment, only this software is present, and this software is not present when outside the isolated space that comprises the virtual environment. Virtual environments are useful because they allow us to use specific software for specific projects. For example, if a model was built using Python 2, but bugs out in Python 3, we can create a virtual environment to run Python 2 and run the model in there without replacing the Python version on our whole machine. Advanced Python users do EVERYTHING in virtual environments.\n",
    "\n",
    "A kernel is a special case of a virtual environment that also includes the variables/objects you create in a particular Python session.\n",
    "\n",
    "Installing different Python versions on your computer\n",
    "\n",
    "One of the primary uses of a virtual environment is to run code on a specific (usually older) version of Python without changing the version installed in the global environment on your computer. The best package for downloading and installing different versions of Python is called pyenv. To install pyenv, follow the instructions here: https://github.com/pyenv/pyenv\n",
    "\n",
    "\n",
    "To install a new version of Python using pyenv, first take a look at the list of available Python versions for your computer by typing (in either the Mac terminal or in the Windows command line):\n",
    "\n",
    "pyenv install -l\n",
    "\n",
    "Choose the version you need (say for example version 3.6.10), and type\n",
    "\n",
    "pyenv install 3.6.10\n",
    "\n",
    "Now the version of Python you need for your virtual environment should be installed and ready to use in the next step. Note that although this last command downloaded and installed this version of Python, it did not set this version as the default on your computer. If you want to make this version your global environmental default, you can next type pyenv global 3.6.10, however that's neither required nor recommended unless you want to update Python on your computer.\n",
    "\n",
    "The pipenv package\n",
    "\n",
    "There are many ways to use virtual environments in Python, but the pipenv package is the most straightforward and is one of the most widely used tools.\n",
    "\n",
    "In the terminal, type pip install pipenv\n",
    "\n",
    "Projects\n",
    "\n",
    "The idea of a project is that everything you need should be in one folder on your computer. That includes all scripts and notebooks, all local data files, and all supporting files (like pipfiles and .env files, which we will discuss a bit later). In addition, a project sets a virtual environment in the same folder to control the Python version and the associated packages.\n",
    "\n",
    "First we create the project folder. In the terminal, type pwd. This shows the directory the terminal is currently pointing to. If you are using GitHub, you created a new folder when you cloned a repository, and this folder can be considered your project folder.\n",
    "\n",
    "Choosing the Python version for this project\n",
    "\n",
    "Before we can install any packages, we must choose and install the version of Python we want. (If we install packages first, it will default to the latest version of Python, and we won't be able to change it without having to start a new project from scratch).\n",
    "\n",
    "Let's go for a version of Python 3.11. In the terminal, type:\n",
    "\n",
    "pipenv --python 3.11\n",
    "\n",
    "Check out the output. pipenv also automatically created a file called a Pipfile, which we will look at in a minute.\n",
    "\n",
    "Working outside and inside the virtual environment\n",
    "\n",
    "Although you've created the virtual environment, you are still working in your computer's default global environment until you activate the virtual environment. For starters, in the terminal, type python. Notice the version number of your global version of Python. Then to exit Python and return to the terminal prompt, type quit().\n",
    "\n",
    "Next let's activate the virtual environment, and then run Python again to see what happens.\n",
    "\n",
    "The first way to activate a virtual environment is with the following command, typed in the terminal (making sure the prompt says you are in the project's folder):\n",
    "\n",
    "pipenv shell\n",
    "\n",
    "Notice the prompt used to say (base), and now it has the name of your project folder in parentheses. That indicates you are now working in a virtual environment. Type python again. Note the version number now! You have one version of Python just for this virtual environment, and an entirely different one for your global system. Type quit() to return to the prompt.\n",
    "\n",
    "When you are in the terminal and you've used pipenv shell to enter the project folder's virtual environment, type exit to return to the global environment.\n",
    "\n",
    "The second way to run a virtual environment is to type pipenv run followed by the command you would like to run inside the virtual environment. This approach is more efficient when you only want to run one command, or a small number of commands one at a time, inside a virtual environment. Type pipenv run python. You'll see the same 3.7.10 instance of Python. Now type quit(). In this case, because we've never used pipenv shell, we are already in the global environment and we do not have to now use exit to return to the global environment.\n",
    "\n",
    "Installing packages in the virtual environment\n",
    "\n",
    "To install Python packages in a virtual environment only, make sure you are in the project folder and use the terminal to type pipenv install followed by the name of the package you want to install. It's exactly the same as pip install, but it only installed the package for a specific virtual environment instead of the global one.\n",
    "\n",
    "Try these:\n",
    "pipenv install numpy\n",
    "pipenv install requests\n",
    "\n",
    "Let's test this: type pipenv run python. Then on the Python prompt type import numpy. Then type import requests. These two should run without error. Then type import pandas. You'll get an error because we haven't installed pandas in this virtual machine!\n",
    "\n",
    "We saw the folder where this code exists in the global environment, but to see where the packages live in the virtual machine, type import sys, then sys.path. Find the address that ends in site-packages. Copy this address, then exit Python, and take a look at this folder. You'll see the packages that we just installed here and a few additional packages that base Python, requests, and numpy depend on, but no other packages yet.\n",
    "\n",
    "By the way, to uninstall a package in a virtual environment, you can type uninstall instead of install. Try: pipenv uninstall requests\n",
    "\n",
    "Look at the Pipfile and Pipfile.lock\n",
    "\n",
    "When you created the virtual environment with Python 3.11, pipenv created two files: Pipfile and Pipfile.lock. These two text files contain information about the entire contents of the virtual environment, including the Python version and the installed packages. Open the Pipfile in any text editor. It should look like this:\n",
    "\n",
    "[[source]]\n",
    "url = \"https://pypi.org/simple\"\n",
    "verify_ssl = true\n",
    "name = \"pypi\"\n",
    "\n",
    "[packages]\n",
    "numpy = \"*\"\n",
    "\n",
    "[dev-packages]\n",
    "\n",
    "[requires]\n",
    "python_version = \"3.11\"\n",
    "\n",
    "Under [[source]] this file describes the repository where pipenv will look be default for new packages. PyPI.org is the Python Package Index, the repository that stores any package downloadable with pip. Under [[packages]] you see a list of all of the packages you installed (requests isn't here because we uninstalled it). The * refers to the latest version available, but if we want a specific version we can include that in the pipenv install command, like this: pipenv install requests==2.22.0. After running this command, take another look at the Pipfile and you will see the version number listed instead of a *. That means that any time we rebuild this virtual environment in the future, the specific 2.22.0 version of requests will be installed instead of the latest version.\n",
    "\n",
    "There is also a file called Pipfile.lock. Take a look. This file contains the same information as the Pipfile, and additional information like hashes and markers. That's important for installation instructions, but it's all generated and handled automatically and we don't have to ever worry about the Pipfile.lock file. Best to just leave it alone.\n",
    " \n",
    "Run JupyterLab in a virtual environment\n",
    "\n",
    "It's one thing to run Python from the command line in a virtual environment, but nobody uses Python this way. Here's how to use JupyterLab inside a virtual environment:\n",
    "\n",
    "First, JupyterLab is itself a package we must download into the environment, so type: pipenv install JupyterLab\n",
    "\n",
    "In addition, JupyterLab uses a package to manage its kernels called ipykernel, so install that too: pipenv install ipykernel\n",
    "\n",
    "Finally, we need to get JupyterLab's kernels to understand the virtual environment as another kernel choose from as an alternative to the global Python 3 environment. Register the virtual environment in JupyterLab by typing: pipenv run python -m ipykernel install --user --name=my-venv Here we've elected to name the virtual environment's kernel \"my-venv\" but we could have named it anything we like.\n",
    "\n",
    "Next, open JupyterLab inside the virtual environment by typing: pipenv run JupyterLab. You will see my-venv (or whatever you named your virtual environment) as an option for the kernels you can use to run a notebook. When you select this option, the notebook will run on the version of Python you installed on the virtual machine and will only be able to import the packages you installed.\n",
    "\n",
    ".env file and environmental variables\n",
    "\n",
    "A .env file (a \"dot E-N-V\" file) is a plain text file that is used for storing sensitive pieces of information such as passwords and access keys. We will use these files a lot later in this course when we talk about APIs and again when we talk about databases. A .env contains environmental variables, which are pieces of data that load into the virtual environment used by a script or notebook without having to be defined in the code itself. Think of it this way: say we are trying to use Python to access a system that requires a password. But if we are trying to keep a password secret, it's not good to explicitly type something like password = 'idancetobieberlikenooneswatching' into the code. But if we define this password as an environmental variable when we create a virtual environment, then the virtual environment tells Python what the password is without ever having to be typed out.\n",
    "\n",
    "Because the .env file is just a plain text file, you can create a .env file in a number of different ways. One way that should work for both Mac and Windows machines uses JupyterLab. Within JupyterLab, make sure the left-hand file navigator window is showing your project folder. Then select File, New, Text File. Then select File and Save Text As. Replace 'untitled.txt' with '.env' and save it. If you use another method to create this file, just make sure the file is named .env and not accidentally .env.txt.\n",
    "\n",
    "Once the file is saved, you can define the environmental variable by writing the word you will write in the code, an equal sign, and the secret value associated with the word. For example, type the following into the .env file and save the file: password=idancetobieberlikenooneswatching. You can define as many environmental variables as you want, just put each one on a new line inside the .env file.\n",
    "\n",
    "Please note that on most computers .env is considered to be a hidden file, so it won't show up by default when looking for files unless you set an option on your computer to display hidden files (Shift + Command + . on a Mac, View > Show > Hidden items within File Explorer on Windows). Also, it is very important that you never include a .env file in the files you post to GitHub or to Docker Hub. Malicious actors know to look for files called .env and will try to exploit the information contained in these files. If you initialized your GitHub repo with a .gitignore file, .env is included by default in this file.\n",
    "\n",
    "Now return to the terminal. If it is still showing the log for JupyterLab, press Shift + Command to bring back the terminal prompt. Now if you rerun pipenv run JupyterLab (or any pipenv run command) you should see a message that reads \"Loading .env environmental variables\".\n",
    "\n",
    "Start a notebook using the virtual enviroment as a kernel, and type import os and password = os.environ['password'] into the first cell. Note that os is part of base Python, so it doesn't have to be separately installed. These two commands create a Python variable called password that contains your secret password, but does not require you to ever type out your password in the notebook.\n",
    "\n",
    "Thursday, August 24\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Creating a GitHub Repo: https://docs.github.com/en/get-started/quickstart/create-a-repo\n",
    "\n",
    "Setting up Git: https://docs.github.com/en/get-started/quickstart/set-up-git\n",
    "\n",
    "Cloning a repository (we'll be using the Command Line method): https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository\n",
    "\n",
    "Here's an excellent eBook for advanced use of Git and GitHub, free for UVA students: https://learning.oreilly.com/library/view/learning-git/9781098133900/\n",
    "\n",
    "\n",
    "For every class we will use the following workflow:\n",
    "\n",
    "Open the terminal on your computer and use cd to navigate to your project folder (that has the same name as your GitHub repository)\n",
    "Type git pull origin main\n",
    "The word \"origin\" within git pull refers to your GitHub repository (or wherever the git clone command was instructed to originally told to download the code from). The word \"main\" refers to your local project folder. A couple side notes about \"main\":\n",
    "An advanced use of GitHub uses something called branches in which you can create a copy of the code to build out a specific extension before merging it back in with the main code. In this case, you would supply the name of the branch here instead of main. Branching is important when working collaboratively as part of a software development team, but we won't cover that practice here.\n",
    "Until very recently, the default name for the main branch of a git repository was \"master\". The tech community collectively decided to switch this word to \"main\" due to the connotations to slavery. It's part of a larger effort to strike the words \"master\" and \"slave\" from tech jargon. So you might see the word \"master\" in documentation and in older code examples. You can read more about this change here: https://www.wired.com/story/tech-confronts-use-labels-master-slave/\n",
    "\n",
    "If you are working by yourself on one computer, then using the git pull command will probably simply show you a message that your local repository is up to date with the GitHub repository. Still, it is best practice to habitually start by using the git pull command. If you are working on multiple devices or as part of a team, git pull brings the most current code into your local installation.\n",
    "At this point, work on your code locally, making sure to save your work often. Your code will include Python scripts and notebooks, Dockerfiles, and other kinds of files.\n",
    "If there are any files you do not want to share, open the .gitignore file (which is a plain text file) and type the names of those files here.\n",
    "Once you have completed your work for one session, type git add *\n",
    "This command adds the files to the set of changes you are interested in saving and sharing publicly on GitHub. To add all the files in the directory to this set, type a *. If you only want a subset of those files, you can just type those files.\n",
    "However, I recommend always using git add * and using the .gitignore file to specify the files you don't want to go to GitHub.\n",
    "Commit the changes by typing: git commit -m \"short but descriptive message here\"\n",
    "Creating a git commit does two things. First is allows you to provide short messages to describe your progress during your work session. The full list of commits ultimately provides a narrative of all the work you've accomplished on this project.\n",
    "Second it creates an archived version of the project, so if things completely break down the road, you can reset the code to an old commit as a last resort.\n",
    "To copy your updated work to your GitHub repo, type: git push origin main\n",
    "If Git asks you to supply your GitHub user name and email address, you can provide those by copy-and-pasting the code Git provides in the error message.\n",
    "If Git asks you to supply your GitHub password, confusingly, it does NOT want your password. Instead, follow these steps:\n",
    "Go to https://github.com/ and sign in\n",
    "Click on the icon in the top-right corner of the screen to see the menu for your account. Click on Settings.\n",
    "At the bottom of the left-hand tool bar, click Developer Settings.\n",
    "Click on Personal Access Tokens, then Tokens (classic).\n",
    "Click on \"Generate New Token\" then \"Generate New Token (classic)\". Sign in again if prompted to do so.\n",
    "Under Note, write a short description of the project this token is for.\n",
    "You can set expiration for whatever you want, even unlimited. If you want to be more secure and choose an expiration date, you will have to repeat these steps once that date is passed.\n",
    "Click on all of the boxes, then click Generate Token at the bottom.\n",
    "GitHub will now display your personal access token. Copy it, and paste it somewhere secure (such as in an .env file, which we will cover later in the course)\n",
    "You need to paste this token instead of your GitHub password when Git asks for your password when conducting a git pull."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214505c4-cceb-433a-a846-91875c18f695",
   "metadata": {},
   "source": [
    "12. Using volumes to save your work and data inside a container\n",
    "\n",
    "Whenever you close a Docker container, all of the files and data that exists inside the container gets wiped out. To save files, you can use the docker cp command described above to copy each file from the container to permanent local storage on your computer. But data is often stored in the running memory of a computer system instead of in a file. If you want data such as Python variables and database records to persist after a container is closed, the best approach is to define and use a Docker volume. Persist means that all the data that exist inside a container when it shuts down get saved and reloaded when the container is restarted.\n",
    "\n",
    "There are two steps to using a volume. First we have to define the volume, then we attach the volume to a Docker image when we use the docker run command to launch a container.\n",
    "\n",
    "To create a new volume, type\n",
    "\n",
    "docker volume create myvolume\n",
    "\n",
    "The final word in this command can be any name you choose to give the volume. Just remember the name you chose so that you can attach it to containers.\n",
    "\n",
    "What exactly is a volume? That's a little hard to understand (it took me a while to wrap my head around it, at least). Without getting too technical, when you install and launch Docker on your computer it automatically and silently starts a container on your computer that runs Ubuntu Linux and contains all of the background code Docker needs to run various Docker commands. This Ubuntu container is always supposed to run only in the background and support the user-facing sides of Docker. When you create a volume, Docker creates a new folder in this background Ubuntu container and saves all of the data from one of your containers in this folder. All of the Docker syntax regarding volumes provides shortcuts to accessing the background container and either pulling data out of this folder or saving data into this folder.\n",
    "\n",
    "At any rate, now that you have created a volume, you can see it listed in the Docker Desktop dashboard by clicking the Volumes tab.\n",
    "\n",
    "Now that we have created a volume, we can attach it to a Docker container with the docker run command. Let's continue working with the example we used above with the myfirstdocker image here, but you can use any image you want with a volume.\n",
    "\n",
    "If the container you started is still running, start by closing it. One way to close the container is to type docker ps to see a list of running containers. If you see a container you want to shut down, find it's nickname (such as elastic_hertz), then type\n",
    "\n",
    "docker stop elastic_hertz\n",
    "\n",
    "Next we will relaunch the container, this time with attaching a volume with the -v flag on the docker run command. Type:\n",
    "\n",
    "docker run -p 8889:8888 -v myvolume:/notebooks myfirstdocker\n",
    "\n",
    "The -v myvolume:/notebooks option has three parts. First -v tells docker run that you are attaching a volume to this container. myvolume is the name of the volume you've just created that stores data on the background Docker container. The colon specifies a mapping from the volume to a specific folder inside your container. Recall that when we wrote the Dockerfile for this container we used the WORKDIR command to create a folder called /notebooks. Here we tell Docker to load the contents of myvolume into the /notebooks folder when we launch the container, and then to save the contents of /notebooks into myvolume when we shut down the container. If we write -v myvolume:/notebooks everytime we run the container, we can save our work and we can save the data inside the container.\n",
    "\n",
    "Let's try it out. Now that the container is running with an attached volume, open a new notebook, type \"This is a test!\" into the first cell and set the cell to display markdown, and save the notebook with some name. Then exit JupyterLab and close the container. Next, restart the container using the exact same command: docker run -p 8889:8888 -v myvolume:/notebooks myfirstdocker You should see the notebook you just saved reappear in the left-hand file window.\n",
    "\n",
    "13. Databases using Docker\n",
    "\n",
    "There are two ways to store data on a computer. You can save data in a file on your hard drive: that's what you do when you save data in a CSV file, an Excel file, a Stata file, or another specific file type. But you can also save data in the working memory on your computer, just like any program that runs or any container running on a specific port. A database management system (DBMS) is a program that saves data in your computer's working memory and also provides a mechanism for accessing the data. The most famous mechanism for database access is called the Structured Query Language (SQL). We will talk all about SQL later in this course.\n",
    "\n",
    "Different DBMSs work with different kinds of data. The most common kind of DBMS works with relational databases: data that are stored in a series of tables with rows and columns, where different tables can be merged together based on a set of shared columns. Like SQL, we'll go over relational databases in depth in a few weeks. There are other kinds of databases.  A document store works with data that usually contains a lot of text. A graph database works with data that connects observations to other observations in a network.\n",
    "\n",
    "When we build the Congress Transparency dashboard, we will use a relational database to store the name, committees, votes, and other information about each member of Congress. We will use a document store to organize the text of the bills before Congress, and we will use a graph database to store the network formed by the cosponsorships between Members of Congress.\n",
    "\n",
    "There are many options for DBMSs. Some software is proprietary and expensive, but a lot of DBMS software is free, open-source, and just as good or better than the proprietary options. We will use PostgreSQL for relational databases, MongoDB for document stores, and neo4j for graph databases.\n",
    "\n",
    "Databases are notoriously difficult to install. In the master's version of the class, in which we do everything in the global environment, I dread database week because everyone will get stuck on some idiosyncratic problem with installing one of the databases on their computers. But Docker makes this all much easier because Docker Hub contains official images for each of these DBMSs. Once we feel comfortable with Docker, it pays off immediately when we want to use databases.\n",
    "\n",
    "Our goal here is to run a Docker container for PostgreSQL, MongoDB, and neo4j. For each one we need to specify\n",
    "\n",
    "particular environmental variables (user names, passwords, etc) that we will save in an .env file,\n",
    "a mapping from the default port of the DBMS to a port on our computer,\n",
    "and a volume for the data in each database.\n",
    "\n",
    "First, let's create Docker volumes to persist the data in each DBMS:\n",
    "\n",
    "docker volume create postgresdata\n",
    "docker volume create mongodata\n",
    "docker volume create neo4jdata\n",
    "\n",
    "The official Docker image for PostgreSQL is here: https://hub.docker.com/_/postgres. Postgres requires a username and password to access the data. It assumes the username is \"postgres\", but it requires us to choose a password by defining it as an environmental variable named POSTGRES_PASSWORD, which we save in our .env file. By default Postgres runs on port 5432 inside the container, which we can map to 5432 or to another port on our computer. Postgres saves data by default in a folder in the container named /var/lib/postgresql/data, which we can map to the volume we created above. If we type psql after the docker run command we can access the PostgreSQL command line directly, but otherwise PostgreSQL will run in the background for as long as its Docker container is running, which is fine for us as we will be using Python and not the command line to work with PostgreSQL.\n",
    "\n",
    "After adding the needed environmental variables to our .env file, we can run PostgreSQL by typing:\n",
    "\n",
    "docker run -p 5432:5432 --env-file=.env -v postgresdata:/var/lib/postgresql/data postgres:latest\n",
    "\n",
    "The official Docker image for mongoDB is here: https://hub.docker.com/_/mongo. MongoDB requires the following environmental variables for setting the username and password required to access the database: MONGO_INITDB_ROOT_USERNAME, and MONGO_INITDB_ROOT_PASSWORD. We add these environmental variables to our .env file. Mongo runs on port 27017, which we can map to any open port on our computer. Mongo saves data in a folder inside its container named /data/db, which we map to the mongodata volume we created. If we want to work with mongodb directly on the command line, we can write mongo at the end of the docker run command. Otherwise, mongodb will run silently, which will be fine for us later in the semester because we will use Python to access this running mongodb instance.\n",
    "\n",
    "After adding the needed environmental variables to our .env file), we can run MongoDB by typing:\n",
    "\n",
    "docker run -p 9000:27017 --env-file=.env -v mongodata:/data/db mongo:latest\n",
    "\n",
    "Here we changed the port to 9000 just to avoid any issues with a port with 5 digits.\n",
    "\n",
    "The official Docker image for neo4j is here: https://hub.docker.com/_/neo4j. neo4j requires us to set a password for a user named \"neo4j\". We can set this password in the .env file by typing\n",
    "\n",
    "NEO4J_AUTH=neo4j/your_password\n",
    "\n",
    "replacing your_password with one we choose. neo4j saves its data inside a folder named /data inside the neo4j container, and we will map the neo4jdata Docker volume we created to this folder. By default, neo4j runs in port 7687, which we can map to 7687 on our computer. If we want to work with neo4j directly on the command line, we can write neo4j at the end of the docker run command. Otherwise, neo4j will run silently on port 7687, which will be fine for us later in the semester because we will use Python to access this running neo4j instance.\n",
    "\n",
    "After adding the needed environmental variables to our .env file, we can run neo4j by typing:\n",
    "\n",
    "docker run -p 7687:7687 --env-file=.env -v neo4jdata:/data neo4j:latest\n",
    "\n",
    "14. Using Docker compose to launch and link up all of these containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69367ad3-33c1-4856-953f-7791cef873b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
