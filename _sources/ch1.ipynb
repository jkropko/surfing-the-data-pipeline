{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Yourself Unstuck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{contents} Table of Contents\n",
    ":depth: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Modeling and analytics get the lion's share of attention in the classroom in data science programs, but in the real world, data is almost never ready to be analyzed without a great deal of work to prepare the data first. [This article in Forbes](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#3ebb8cd06f63) describes a survey of data scientists in which the respondents claim to spend nearly 80% of their time collecting and cleaning data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg\" alt=\"Apologies for the pie chart, it will be the last one we see in this course\" width=\"600\"/> \n",
    "<p style=\"text-align: center;\"><b>Source:</b>\n",
    "<a href=\"https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=ee9be706f637\">'Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says' by Gil Press</a>\n",
    "</p></center>\n",
    "\n",
    "The [Anaconda State of Data Science](https://www.anaconda.com/resources/whitepapers/state-of-data-science-report-2022) report in 2022 finds that data scientists \"spend about 37.75% of their time on data preparation and cleansing. Beyond preparing and cleaning data, interpreting results remains critical. Data visualization (12.99%) and demonstrating data’s value through reporting and presentation (16.20%) are essential steps toward making data actionable and providing answers to critical questions. Working with models through selection, training, and deployment takes about 26.44% of respondents' time.\"\n",
    "\n",
    "Whatever the exact numbers, the fact remains that as a data scientist you will be spending a great deal of time collecting and preparing data in advance of an analysis, and a lot of time interpreting and communicating findings. The modeling itself will be a comparatively small part of your work.\n",
    "\n",
    "And that's why traditional academic training for data science can lead students to be underprepared. In many classroom settings, a professor will give the students a cleaned dataset and will task the students with running a predictive model using the data. But working with data that is ready for analysis from the outset is rare; in practice, before this kind of modeling can be done, data scientists have to identify the data they need, get access to the data, and find a way for the data to be loaded into a Python environment. That might mean loading data from files or APIs, or extracting data from webpages, or accessing a database. Then data scientists have to manipulate the data so that it is exactly in the format an analytic model expects, and that often involves mastery of SQL and the `pandas` package to manipulate rows and columns of a data matrix, merge and pivot data tables, handle missing data, recode and combine categorical values, along with a myriad of other tasks. And after finally getting results, yet more work is needed to communicate the output in a clear way that allows that audience to draw fast and accurate conclusions using data visualizations and interactive dashboards.\n",
    "\n",
    "The phrase **data pipeline** refers to all of the steps needed to go from raw, messy, original data to data that are ready to explore and analyze to the products we share to communicate results to an external audience. \n",
    "\n",
    "The goal of this book is to make the steps in the data pipeline other than modeling - acquiring, wrangling, and communicating data - easier, faster, less frustrating, and more enjoyable for you. The techniques we will discuss are not the only ways to accomplish a task, but they represent fast and straightforward ways to do the work using Python.\n",
    "\n",
    "This book is divided into three parts beyond this introductory chapter:\n",
    "\n",
    "<ol start=\"1\">\n",
    "    <li> <b>How do we acquire data?</b></li>\n",
    "</ol>\n",
    "\n",
    "  * From external files with flat, tabular structure (Chapter 2)\n",
    "  * From JSONs, often from APIs (Chapters 3 and 4)\n",
    "  * From web-scraping using `beautifulsoup` (Chapter 5)\n",
    "  * From local or remote access to an SQL or NoSQL database (Chapters 6-7)\n",
    "\n",
    "<ol start=\"2\">\n",
    "    <li> <b>How do we clean/wrangle/manipulate data to prepare the data to be analyzed?</b></li>\n",
    "</ol>\n",
    "\n",
    "  * With SQL queries (Chapter 7)\n",
    "  * With `pandas`, including merging and reshaping dataframes (Chapters 8-9)\n",
    "\n",
    "<ol start=\"3\">\n",
    "    <li> <b>How do we explore data and communicate findings?</b></li>\n",
    "</ol>\n",
    "\n",
    "  * With summary and descriptive statistics tables (Chapter 10)\n",
    "  * With static visualizations using `matplotlib` and `seaborn` (Chapter 11)\n",
    "  * With interactive visualizations using `plotly` (Chapter 12)\n",
    "  \n",
    "However, prior to launching into the data pipeline, we have to talk about the single most important skill for a data scientist: **how to get yourself unstuck**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being a Data Scientist Means Knowing How to Get Yourself Unstuck\n",
    "Most of you reading this book would describe yourselves as beginners with Python code and the practice of data science. Being a beginner is a wonderful thing. As a beginner, the rate at which you learn new skills is faster than it will be at any other point in your career. And the feeling of accomplishment you will feel by writing working code can and should be profound. But one drawback of being a beginner is the feeling of being judged or viewed as insufficient in comparison to more experienced coders who have much more of this programming language memorized. [Imposter syndrome](https://en.wikipedia.org/wiki/Impostor_syndrome), a persistent feeling of doubt in one's skills and the feeling of being a fraud, is especially common for students who are just starting out in a new field. And it's only natural to look at people who type and execute code at furious speeds and to think of yourself as an imposter, especially when you keep getting errors when you try to execute your code.\n",
    "\n",
    "But here's the secret: problems, errors, and anomalies in your code are inevitable. And that's true whether you are a beginner or a programmer with decades of experience. It would be accurate to describe a workday of a data scientist as moving from one error to the next to the next. It is simply the nature of using a big programming language like Python that includes so many user-written and open source extensions. So you will feel at times like all you do is generate errors with your code. This feeling is natural, and it is a universal feeling that all data scientists share regardless of their experience level. What sets experienced data scientists apart is their skill in using help resources fix bugs and to to get to the right answer, **as quickly as possible**. \n",
    "\n",
    "As a student, you often have access to an instructor who can answer questions and help you work out the bugs in your code. But as a practitioner you might not have access to someone with the experience and time to help you with your code in a one-on-one way. Some proprietary software sells access to helplines and consulting, but open source projects like Python depend on the community of Python users to provide this support. The more popular an open source software package the bigger the community and the better the resources to help programmers solve problems. And it just so happens that [Python is the most popular programming language in the world](https://linuxiac.com/python-the-most-popular-programming-language/), and there are some incredible resources available to anyone working with Python. So the most important skill to master to become an advanced data programmer is participating in this community to quickly find answers to the problems that arise as you code.\n",
    "\n",
    "In this chapter, we will discuss the various methods and resources at your disposal for finding help. Some of the methods provide quick assistance but focus on smaller issues, some methods are slower but help you solve harder problems, and some methods are only useful for certain situations. In general, it is best practice to start with quicker methods and to only move to slower methods if you can't find a solution with a quicker one; so there is an *order* in which you should employ each resource. If you practice using these help methods and memorize the order in which to try using each one, you won't have any trouble squashing all the bugs in your code.\n",
    "\n",
    "<center><img src=\"http://phdcomics.com/comics/archive/phd011406s.gif\" width=\"600\">\n",
    "<p style=\"text-align: center;\"><b>Source:</b>\n",
    "<a href=\"https://phdcomics.com/comics/archive.php/archive/tellafriend.php?comicid=673\">Ph.D. Comics, \"Debugging\", by Jorge Cham</a>\n",
    "</p></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Places to Go For Help, In Order\n",
    "There are many resources available to you. I suggest using the following resources in a particular order:\n",
    "\n",
    "1.  Reading and understanding Python errors\n",
    "2.  Python documentation\n",
    "3.  Google\n",
    "4.  Stack Overflow\n",
    "5.  Large language models such as [ChatGPT](https://chat.openai.com/)\n",
    "6.  Places to connect with the larger worldwide community of Python programmers, such as PySlackers and Python Discord, internet relay chat (IRC) rooms, and various Python mailing lists\n",
    "\n",
    "Several of these resources are ones which are officially recognized and recommended by the Python Software Foundation, the nonprofit organization that maintains the official Python distribution. See  https://www.python.org/community/ for more information about these resources.\n",
    "\n",
    "Many new programmers use Google and Stack Overflow as their first options for getting help, and recently many people feel the pull to consult a ChatBot right away. I strongly suggest that you do **not** look to Google, Stack Overflow, or a ChatBot before trying to use the official Python built-in documentation or try to understand an error message you receive. The documentation and error messages aren't as intuitive and easy to access as Google or ChatGPT, but the answers that the official documentation will provide are guaranteed to be correct and specific to the functions you are trying to use, if you know what those functions are. In contrast, for all of the strengths of Google, Stack Overflow, and ChatBots, they can be wrong or misleading because there is a lot of information out there that isn't especially helpful. Defaulting to these resources as a first recourse will slow you down a great deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Reading and Understanding Python Errors\n",
    "Usually, when you are stuck, it's because you ran some code that resulted in an error. Reading and *understanding* the error is the single most important and useful way to get yourself unstuck. These errors are not aesthetically pleasing, and they are written in technical language, but the intention of these error messages is to tell you exactly what went wrong. Many new Python users skip reading the errors entirely, which is a shame because the error might indicate exactly the fastest way to solve the problem.\n",
    "\n",
    "There are two reasons why people skip over reading the errors:\n",
    "\n",
    "1. The error appears in a big pink box with pea green and cyan text inside of it. It's pretty ugly.\n",
    "\n",
    "2. The first several lines of code are reserved for a function's **traceback**. The traceback is an attempt to isolate the particular line of code within a larger function that causes the error. However, the traceback is often very technical and not especially useful. After reading the first few lines of the traceback, many people give up on the entire error message. \n",
    "\n",
    "But the useful part of the error message occurs **at the bottom of the message**. The traceback is useful when developing new functions and debugging original software. But the vast majority of the time, whenever you are using pre-programmed methods and functions, only the bottom of the error message matters.\n",
    "\n",
    "For example, in chapter 2 we will discuss loading electonic data files. Many things can go wrong. One error we will contend with comes from the following code:\n",
    "```python\n",
    "from pandas import read_csv\n",
    "url = \"https://raw.githubusercontent.com/jkropko/DS-6001/master/localdata/anes_example_toplines.csv\"\n",
    "anes = read_csv(url)\n",
    "```\n",
    "What this code is supposed to do is not important at the moment. It produces the following error output:\n",
    "![](error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of this output helps me understand my mistake until I reach the final line that begins `ParserError`. With a little bit of experience, I can understand this error: it says that it expected the fifth row of the data file (\"line 5\") to have one column (\"Expected 1 field\"), but instead it found 168 columns (\"saw 168\"). That's useful - it tells me that the first four rows of the data file do not contain the data I need, probably because the data authors put text or citation information at the top of the file - but I had to wade through dozens of lines of technicality to arrive at the useful error message. \n",
    "\n",
    "One way to make reading error messages easier is to turn off the traceback by loading the `sys` library and issuing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.tracebacklimit = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when I use the same code, I get a different error output:\n",
    "\n",
    "![](error2.png)\n",
    "\n",
    "I can't say that this error output is pretty or intuitive in and of itself, but relative to the previous error message this output has some nice properties. First, it is shorter (and there's less pink). Second, and most importantly, the useful part of the error message appears much closer to the top of the output, so there's less scrolling involved and the error takes up less space.\n",
    "\n",
    "To turn the traceback back on, simply type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.tracebacklimit = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using the Built-in Python Documentation \n",
    "Packages, modules, classes and functions in Python have built-in documentation that you can display directly in the console or in the output of a notebook. These built-in documentations are called **docstrings**. The docstring, and not Google, should be the first place to look for help with specific, pre-built Python code as it conveys guidance directly from the code's authors. In contrast, going to Google first often turns up blogs and posts in which another secondary user describes the code. ChatBots like ChatGPT are built in large part from scraping these secondary uses from various webpages. Either way, these approaches are more prone to misinterpretations and mistakes.\n",
    "\n",
    "### Using a Console Window in JupyterLab\n",
    "In my experience working with new Python programmers, beginners are very reluctant to use the docstrings. That's unfortunate because docstrings are fast and accurate ways to solve issues with using existing code.\n",
    "\n",
    "One reason for this reluctance is that calling up a docstring is annoying in a Jupyter notebook. Ideally, you should be working with two windows side by side: one for the notebook or script that contains your code, and one for viewing docstrings. Another reason new users don't use docstrings is that the help documentation isn't accessable until the packages that contain the relevant functions are loaded into the workspace.\n",
    "\n",
    "The following is a procedure you can use to make viewing docstrings easier with Jupyter notebooks. With a little practice it can become a good habit that saves you a great deal of time in the long-run:\n",
    "\n",
    "1. Use JupyterLab and open the notebook you are working on.\n",
    "\n",
    "2. Run the entire notebook (or at least everything up to an error you are trying to fix).\n",
    "\n",
    "3. Right click somewhere in the notebook and select \"New Console for Notebook\".\n",
    "\n",
    "4. Click on the tab for the new console window and drag it to the right so that it occupies the left-side of the screen.\n",
    "\n",
    "I strongly suggest using JupyterLab (not just Jupyter Notebook), which is available as part of [Anaconda Navigator](https://docs.anaconda.com/anaconda/navigator/) or as a stand-alone package that can be installed with `pip install jupyterlab`. JupyterLab has a number of features to make it easier to use relative to the regular Jupyter Notebook interface. One of those features is the ability to open consoles with the same kernel as an open notebook, and another feature is the ability to move tabs so that they occupy a portion of the screen to the left or right, on the top or bottom.\n",
    "\n",
    "The procedure listed above gives you a visible and separate place to view docstrings. It also loads the same kernel as the notebook you are working on, so that if you've imported `pandas`, for example, `pandas` is also loaded into the console so that you can call the docstring for any `pandas` module or function. Keeping this console open will help you more easily integrate calling and reading docstrings into your workflow.\n",
    "\n",
    "### How to Call Up and Read a Docstring\n",
    "There are many different kinds of code objects in Python that have attached docstrings:\n",
    "\n",
    "* A package itself, such as `pandas` or `matplotlib`,\n",
    "\n",
    "* A module: a named subset of the code within a package that focuses on a specific topic. One example is the `matplotlib.pyplot` module which handles all of the functions to display the graphics constructed with other functions within the `matplotlib` package for data visualization,\n",
    "\n",
    "* A function, either within the base Python code such as `print()`, or within a package or module such as `pandas.read_csv()`,\n",
    "\n",
    "* A Python variable that you, the user, has created. Some types of variables have associated **attributes** and **methods**. An attribute is another Python variable that can be extracted from the one you created. For example, if you create a data frame with the `pandas` package and name it `mydataframe`, it has an attribute `mydataframe.columns` that is a list of the names of the columns of your data frame. A method is a function that operates on this data frame. For example, typing `mydataframe.corr()` calculates the correlation between all pairs of columns in the data. (One easy way to tell the difference between attributes and methods is that attributes do not use parentheses, and methods must always use parentheses.)\n",
    "\n",
    "To display the docstring, replace the word \"helpme\" with the name of the relevant package, module, function, or variable in one of the following:\n",
    "\n",
    "* `help(helpme)` displays the entire docstring for the package, module, function, or variable in question. If `helpme` is a user-defined variable, this method displays the attributes and methods available for the variable, if any.\n",
    "\n",
    "* `helpme?` or `?helpme` displays an abbreviated docstring, as well as the signature (the complete function syntax, including all arguments and their default values). However, if `helpme` is a user-defined variable, this method does not display the attributes and methods available for the variable. \n",
    "\n",
    "* `helpme??` or `??helpme` is the same as `help(helpme)` but shows the internal code for the function and methods if applicable and possible.\n",
    "\n",
    "For example, consider the docstring for the `print()` function, which displays results to the notebook or console output. We can call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function print in module builtins:\n",
      "\n",
      "print(*args, sep=' ', end='\\n', file=None, flush=False)\n",
      "    Prints the values to a stream, or to sys.stdout by default.\n",
      "    \n",
      "    sep\n",
      "      string inserted between values, default a space.\n",
      "    end\n",
      "      string appended after the last value, default a newline.\n",
      "    file\n",
      "      a file-like object (stream); defaults to the current sys.stdout.\n",
      "    flush\n",
      "      whether to forcibly flush the stream.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Prints the values to a stream, or to sys.stdout by default.\n",
       "\n",
       "sep\n",
       "  string inserted between values, default a space.\n",
       "end\n",
       "  string appended after the last value, default a newline.\n",
       "file\n",
       "  a file-like object (stream); defaults to the current sys.stdout.\n",
       "flush\n",
       "  whether to forcibly flush the stream.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Prints the values to a stream, or to sys.stdout by default.\n",
       "\n",
       "sep\n",
       "  string inserted between values, default a space.\n",
       "end\n",
       "  string appended after the last value, default a newline.\n",
       "file\n",
       "  a file-like object (stream); defaults to the current sys.stdout.\n",
       "flush\n",
       "  whether to forcibly flush the stream.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `?print` and `??print` yield the same output because the internal code of `print` is C code, which Python calls but does not display.\n",
    "\n",
    "Docstrings often have **sections** that convey particular information.\n",
    "\n",
    "1. The header (only appears with `help(print)`): tells us that the `print` function exists in the `builtins` module (the module that loads automatically when we launch Python)\n",
    "\n",
    "2. The signature: lists all of the parameters of a function. Each parameter in the signature is set equal to its default value. If the user doesn’t specify the parameter in the function all, it’s set to the default.\n",
    "\n",
    "3. The short description: A one-or-two sentence summary of what the function does. For the `print` function, this summary is \"Prints the values to a stream, or to sys.stdout by default\" which is technical-speak for prints to whatever the output medium happens to be. In a Jupyter notebook that means printing just below a cell of executable code.\n",
    "\n",
    "4. The parameters: **this section is the most useful for learning how to use a function**. The parameters section lists the parameters, in the order in which they appear in the signature of the function, along with information about each parameter. \n",
    "Each parameter is described in a sentence or two to explain what the parameter does.\n",
    "Sometimes the parameters may be noted as either required or optional in a call to the function, and the docstring might also list the type of each parameter: in this case, the `sep` and `end` parameters are denoted as strings.\n",
    "\n",
    "Other docstrings might include sections that describe:\n",
    "\n",
    "1. Returns: describes what the output should look like and contain\n",
    "\n",
    "2. Attributes and methods for a user-defined Python variable\n",
    "\n",
    "3. See also: a list of related functions\n",
    "\n",
    "4. Examples: Examples are meant to be run, not just looked at.  Copy-and-paste\n",
    "the examples into your notebook or script, run the code.  Then see if you\n",
    "can do more things with the given objects than the examples do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Using Google and Other Search Engines\n",
    "Many of us have a habit of going to Google first whenever we have a coding problem. Please make a concerted effort starting right now to break this habit. If you know the functions you need more information about, using the built-in docstrings first the best habit. The docstrings are what the code's authors provide to guide users. If you know where to look within a docstring, you can find the correct answers to your questions quickly. \n",
    "\n",
    "Google is a comparatively slow method for getting help. With open-source environments such as Python, there are many ways to do the same thing, and there are way too many presentations on the internet of any one topic to sift through efficiently. Many of the resources you find this way come from a field or a point of view that differs from your own, leading you to have to work hard to translate the language that explains the method. And some of the information is out of date or simply wrong.\n",
    "\n",
    "Use Google or another search engine if \n",
    "\n",
    "* you don't know the functions you need to do a task,\n",
    "* you don't have a textbook or set of notes that you trust for guidance,\n",
    "* or if the built-in docsting doesn't give you the information you need.\n",
    "\n",
    "The format for Google search terms that usually works best for me is\n",
    "```\n",
    "python (a specific package if you know which one to use) (the function/method you want help with) (additional details)\n",
    "```\n",
    "Starting the search with \"python\" usually narrows the search to Python-specific documentation, message-boards, and blogs. Specifying the package is important when multiple packages can do some version of the same task: for example, if you are trying to use `plotly` to create a bar plot, you will have to sift through results that use alternative packages `matplotlib` and `seaborn` unless you state `plotly` directly. Typing the relevant function or task third narrows those results to more relevant posts. For example, suppose I try to solve the issue that led to the error above when using `pd.read_csv()`. I determined that the problem occurred because the first four rows of the data file are taken up by an unnecessary header placed there by the data authors. I would like to know how to skip these lines. If I turn to the docstring first by typing `?pd.read_csv`, I see that there is a parameter `skiprows` with this description:\n",
    "```\n",
    "skiprows : list-like, int or callable, optional\n",
    "    Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
    "    at the start of the file.\n",
    "```\n",
    "So I can just include `skiprows=4` as a parameter within `pd.read_csv()`. But if I turn to Google, I can issue the following search:\n",
    "```\n",
    "python pd.read_csv how to skip rows at the top\n",
    "```\n",
    "Google will often take you to Stack Overflow (https://stackoverflow.com/) before other websites with coding information. In this case, the first hit is [this Stack Overflow post](https://stackoverflow.com/questions/27325652/python-pandas-read-csv-skip-rows-but-keep-header) which shows me the `skiprows` parameter, but tailors the advice to this particular individual's use case which requires keeping the first row and skipping rows 2 through 11, something not relevant to me in this case. I can get to the right answer this way, but it is slower than going directly to the docstring. \n",
    "\n",
    "Stack Overflow is by far the most widely used and informative repository of coding help and knowledge on the internet. But before we discuss Stack Overflow in more detail, there are other useful and credible websites that often appear when using a Google search to solve a coding problem:\n",
    "\n",
    "* More detailed documentation for specific Python modules, such as:\n",
    "  * https://scikit-learn.org/\n",
    "  * https://pandas.pydata.org/\n",
    "  * https://matplotlib.org/\n",
    "\n",
    "\n",
    "* High-quality blogs:\n",
    "  * https://towardsdatascience.com/\n",
    "  * https://medium.com/\n",
    "\n",
    "\n",
    "* Free content from tutorial websites with a lot more paid content:\n",
    "  * https://realpython.com/\n",
    "  * https://www.geeksforgeeks.org\n",
    "  * https://www.w3schools.com\n",
    "  * https://www.datacamp.com\n",
    "  * https://www.dataquest.io/\n",
    "  \n",
    "There are also many, many more resources and new ones are created all the time. Find the resources you like best: going directly to the most credible links will save you a great deal of time in this effort to get yourself unstuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Avoid Toxicity in Online Communities\n",
    "The next options involve becoming a **responsible, respectful** member of the worldwide community of Python users. Open-source platforms like Python and R depend on a community of volunteers who develop and maintain the tools that we use. All of these people are doing volunteer work for the common good, and that's a beautiful thing.\n",
    "\n",
    "In addition to Python itself, Stack Overflow, Slack, Discord, the Freenode IRC, and mailing lists are online communities for Python users. But, like any online community, there's the potential for a toxic culture to destroy everything.\n",
    "\n",
    "What is a toxic culture? How do you know one when you see one?\n",
    "\n",
    "Toxic cultures are more likely when\n",
    "\n",
    "* members are allowed to be anonymous (see [Lapidot-Lefler and Barak 2011](https://www.dhi.ac.uk/san/waysofbeing/data/health-jones-lapidotlefler-2012.pdf))\n",
    "* and members up-vote and down-vote and comment on each other's contributions (see [Massanari 2015](https://journals.sagepub.com/doi/full/10.1177/1461444815608807)).\n",
    "\n",
    "A culture can be either actively or passively toxic. Actively toxic communities are easy to identify. They encourage and are characterized by overt sexism, racism, bigotry, and calls for violence or other aggression against individuals. Most of the toxicity you will encounter in online programming and data science communities is not actively, but passively toxic. Passive toxicity is characterized by **gate-keeping**: Subtle behaviors that discourage people with less experience, or with some social anxiety, from participating.\n",
    "\n",
    "Stack overflow, IRCs, and mailing lists are notorious for passive toxic behavior. Passive toxicity is a bigger problem for us than active toxicity because actively toxic behavior is usually explicitly banned by codes of conduct, and individuals are often unaware of when they are acting in a passively toxic way.\n",
    "\n",
    "Examples of passive toxic behavior:\n",
    "\n",
    "* **Condescending language**: often people who are trying to earnestly answer a question unthinkingly use language that makes someone feel dumb for asking the question. If you are answering a question on an online programming forum, avoid using words such as \"obviously\", \"clearly\", \"actually\", \"just\", or \"that should be easy\".\n",
    "\n",
    "* **Shaming**: making an implication, even a very slight one, on someone's intelligence or work ethic instead of giving people the benefit of assuming they've tried other avenues before posting. Some subtle examples include saying \"google it\", or \"read the manual\", implying that the questioner must not have tried these methods first. Sometimes people shame others more aggressively on these forums by saying things like \"Learn to debug your own code\" and \"If you don't get this, you have no business being a data scientist\", but hopefully that's becoming more rare.\n",
    "\n",
    "* **Downvotes without explanation**: this can be both confusing and very upsetting to anyone, especially to people with less experience.\n",
    "\n",
    "* **Virtue signaling**: implying that people are superior/inferior because of the language, software, or methods they use. For example: \"Real programmers don't use for loops\", \"You still use SAS?\", and so many memes:\n",
    "\n",
    "<center><img src=\"https://external-preview.redd.it/fU0szLnA4FwuYk3sMNqdf6f21vM_lvC9O-VgjyT11ek.jpg?width=1024&auto=webp&s=7634d370c0a5a46b794e8426850137538eadb615\" alt=\"\" width=\"500\"/>\n",
    "<p style=\"text-align: center;\"><b>Source:</b>\n",
    "<a href=\"https://kieranhealy.org/blog/archives/2019/02/07/statswars/\">'Statswars' by Kieran Healy</a>\n",
    "</p></center>\n",
    "\n",
    "* **Authoritarianism**: Abusing people for failing to follow all of a community's rules for asking questions. For example, a comment that entirely ignores the content of the question but comments \"all questions must provide an example,\" or editing a user's post to remove where they wrote \"Hi everyone\" and \"thanks.\"\n",
    "\n",
    "* **Overzealous curation**: Being very quick to tag a question as a \"duplicate\" without checking to see nuanced ways in which the question comes from a new situation.\n",
    "\n",
    "The result of passive toxicity is that many potential community members choose not to participate in the community because their initial experiences made them feel ashamed, confused, or belittled. In addition, other potential new members observe these negative interactions involving other members, and choose to disengage.\n",
    "\n",
    "Passive toxicity shrinks the community and makes it more homogeneous. Across society, small, homogeneous communities are much more likely to exclude or discriminate against people based on sex, race, class, language and other factors. And that leads to many ethical problems. \n",
    "\n",
    "Remember, most of time when someone is exhibiting passively toxic behavior on a forum, they are well-intentioned but are not thinking about how their responses will be perceived. Very few people deliberately contribute to a toxic online culture, but many people do contribute by failing to empathize with the questioner and by not choosing words carefully. \n",
    "\n",
    "Please keep the behaviors described here in mind when you engage in online communities, and avoid them. Think about your words and place yourself in the position of the original poster before submitting a response, and make the edits that you think would lead to a better online interaction. Don't be afraid to call out other people who behave in these ways. And if you are yourself the person who posts, be aware that these behaviors still happen much too frequently, but know that despite this, most people on these forums mean well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Stack Overflow\n",
    "[Stack Overflow](https://stackoverflow.com) is the most popular and most useful website for help with programming of all kinds. Google searching a Python problem will usually lead to a Stack Overflow post on the same issue. Python is now the most frequent tag for posts on Stack Overflow, and shown in this video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/cKzP61Gjf00\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xffff7bac9fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src=\"https://www.youtube.com/embed/cKzP61Gjf00\", width=\"560\", height=\"315\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a Stack Overflow post that's relevant to your problem can give you both the code and underlying intuition to solve your problem. Or maybe not! Small differences in the situation can make the solution irrelevant to you. Be cautious and don't treat a Stack Overflow post as automatically a definitive answer.\n",
    "\n",
    "### How Stack Overflow Works\n",
    "\n",
    "1. Someone asks a question\n",
    "2. Other people comment on and provide answers to the question\n",
    "3. The person who asked the question replies to the comments, and can choose an answer to mark as \"accepted\".\n",
    "4. People with reputation scores higher than 15 can upvote or downvote questions and answers.\n",
    "5. Reputation points are awarded for asking questions or giving answers that other people upvote, or for having an answer accepted. Points are taken away for downvotes or spam or offensive posts.\n",
    "\n",
    "Going for reputation is an entirely optional activity. If you don't want to worry about it, don't.\n",
    "\n",
    "### Asking a Question on Stack Overflow\n",
    "\n",
    "Okay, so you're stuck. You've combed through the Python documentation, Google, and old Stack Overflow posts, but you haven't found a solution. It's time to consider writing a new question on Stack Overflow.\n",
    "\n",
    "<center><img src=\"https://github.com/jkropko/DS-6001/raw/master/localimages/leia.png\" alt=\"\" width=\"400\"/>\n",
    "<p style=\"text-align: center;\"><b>Source:</b>\n",
    "<a href=\"http://rachievee.com/10-ways-to-learn-wordpress-hooks/\">'10 Ways to Learn WordPress Hooks',  RachieVee: Rachel's Blog.\n",
    "</a>\n",
    "</p></center>\n",
    "\n",
    "This can be frightening. A lot of the time, people answering questions on Stack Overflow can be, well ... huge assholes that cause [real suffering](https://medium.com/@Aprilw/suffering-on-stack-overflow-c46414a34a52). You might choose to avoid posting to Stack Overflow, so as not to support a website that has harbored abuse. That's completely fair. If you do post to Stack Overflow, you are likely to get some very useful responses if you follow some guidelines. There's [a strategy for getting good responses](https://stackoverflow.com/help/how-to-ask). You are more likely to get a good response if you follow these steps:\n",
    "\n",
    "1. **Search Stack Overflow and Google to see if the question has already been answered.** Commenters dislike if the same question is asked repeatedly. This [poor guy](https://stackoverflow.com/questions/27885020/find-all-possible-combinations-of-letters-in-a-string-in-python) got roasted for posing a \"duplicate\" question. (An aside: *Why?* It's not like Stack Overflow is running out of space on their website.  There's an idea that Stack Overflow should be a central repository of knowledge. That means there should be one canonical answer to one question. But people often take this much too far. There are kinder ways to point to an existing answer.) So spend a significant amount of time digging through the internet. If there's something similar, but not quite what you need, you can say so in your post.\n",
    "\n",
    "2. **Write a good title for your post.** A good title is specific about the problem, and also succinct:\n",
    "  * Bad: Problem with matplotlib (not specific)\n",
    "  * Also Bad: How do I place the labels of cars in a scatterplot of the weight and miles per gallon of cars onto the points in the scatterplot using matplotlob 3.3.1 on Python 3.7.4 on Mac OSX 10.14.5? (not succinct)\n",
    "  * Good: How to place labels on top of points in a matplotlib scatterplot?\n",
    "\n",
    "3. **Start the post with a paragraph describing the problem in more detail.** Some good things to include in this paragraph:\n",
    "  * The context of the problem: how did you come across the problem? Describe the overall goal, not the just the buggy step.\n",
    "  * What you've already tried to solve the problem, and what happened.\n",
    "  * What is the expected output? What do you see instead?\n",
    "  * You can write the version of Python you are using, the version of the modules, and the operating system on your computer, in case the problem turns out to be specific to an old version of one of those things.\n",
    "\n",
    "4. If possible, **include code that reproduces the problem.** The code should not simply be the code in your script that isn't working. It needs to be able to work on someone else's computer. That means the code should not depend on any specific data files, and should not contain file addresses that refer to a location on your computer. If possible, only use modules that are easy to get. If the code needs to run on data, can you use something pre-loaded in Python that everyone can access? (There are example datasets included with `scikit-learn`, for example.) Make the code as short and stripped down as possible while still producing the behavior or error that needs to be fixed, and use comments to help people understand the code more quickly.\n",
    "\n",
    "A few additional things to keep in mind:\n",
    "\n",
    "* Be courteous and respectful. Respond to and thank everyone who comments.\n",
    "\n",
    "* Post a follow-up once the problem is solved so that people who come across this page in the future with the same problem know the solution.\n",
    "\n",
    "* Don't ask people to write code for you. It's better to request help with code your provide.\n",
    "\n",
    "* Don't claim you found a bug in Python or in a module. It's a bit rude to the people who programmed the code (who don't get paid).\n",
    "\n",
    "* Don't ask about homework problems. ([Here's an example](https://stackoverflow.com/questions/23098699/calculate-the-mean-of-one-column-from-several-csv-files-in-r) of someone getting called out on this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Using a Large-Language Model (a ChatBot) to Generate and Debug Code\n",
    "\n",
    "Since the release of Open AI's [ChatGPT](https://chat.openai.com/) in March 2023, generative AI and especially chatbots have occupied a great deal of the attention of data scientists in industry and academia. There is far too much to describe about these chatbots in terms of their construction, use, and ethics to fit as a concise discussion in a text like this one. But the main idea you should keep in mind about chatbots is that they are both a necessary part of a data scientist's toolkit, and that they are not any more intelligent than what can be found with a careful search on Google or Stack Overflow. Chatbots are trained on massive amounts of data, largely scrapped from the internet, and with respect to coding the major sources of data are Stack Overflow and GitHub. It's useful to think of a chatbot as a more efficient way to sift through the information that already exists on those websites. \n",
    "\n",
    "Schools are grappling with the question of whether using a chatbot constitutes cheating. But professional software engineering and data science has largely adopted the view that chatbots are fair to use. That said, there is an approach to using chatbots for coding effectively, and knowing the right way to prompt a chatbot to solve coding problems is itself an important skill. So if you are going to use a chatbot, use it correctly. The following discussion outlines some ways to access a chatbot as part of your own workflow and some strategies for writing prompts that can generate the code you need or debug code.\n",
    "\n",
    "### How to Access a ChatBot for Coding\n",
    "The most common way that people are interacting with a chatbot currently is the OpenAI website for chatting with ChatGPT: https://chat.openai.com/. This platform is also free. But there are alternative methods, some of which cost money, that are better suited for an efficient data science workflow. One of the most popular chatbot interfaces in industry is [GitHub Copilot](https://github.com/features/copilot), which integrates with Visual Studio Code. A primary advantage of GitHub Copilot is that it can access your kernel to see and work with the objects you've already created, and it can read the code you've already written. So for example, if you load a dataset into your environment as a `pandas` dataframe and call it `mydata`, GitHub Copilot will already know what `mydata` contains without having to specify it within your prompt. It can save time once you are comfortable using it. There is a cost to use GitHub Copilot, however, and depending on your use case the cost may or may not be worth it.\n",
    "\n",
    "One important point about using chatbots to help you code: it is not a fast method. Generally, getting a useful result from a chatbot requires having a \"conversation\" with the chatbot to narrow in on the desired product. That also involves taking time to study and run the code that the chatbot produces to see what is correct and what is still lacking. The time needed to build to a final output and assess it step by step increases with the amount of work you ask the chatbot to do. As a result, we should not think of a chatbot as a shortcut, and it is still much faster to simply learn how to collect, wrangle, and prepare data, using the chatbot only for particular tasks that slow or stop the progress you can make on your own. \n",
    "\n",
    "Chatbots respond to user-supplied prompts, which can be anything from \"write me a description of different types of fungi in the style of a Jay-Z song\" to specific tasks related to coding and data science. As part of a data science workflow, you might use a chatbot to help you write original code to accomplish a specific task, or to help you debug code you've already written that is generating an error or an unexpected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering for Generating New Code\n",
    "The dream is that we will be able offload the heavy-lift of data wrangling entirely, so that if we simply prompt the chatbot with \"please clean this data\" it will instantaneously generate code that gives us the clean data we want. But that's not how chatbots work at present.\n",
    "\n",
    "We can use a chatbot to do all the work prior to analysis, but it tends to be a slow process, involving a long conversation with a chatbot to flush out the details of what \"clean data\" means for your specific application. Think of this process as similar to supervising a hard-working but inexperienced intern at work. You must provide clear enough instructions to enable the intern to give you what you want and you need to check the intern's work through every iteration of the project. That's not a fast method! So please think of chatbots as a tool, not a cheat-code, and specifically as a tool that is slower and more difficult to use than knowing the code yourself, reading documentation and error messages, and using Google and Stack Overflow.\n",
    "\n",
    "The quickest and most effective method for using a chatbot for data wrangling is to engineer a prompt using these general principles: **Data, Packages, Chunking, Streamline, Knowledge** (or to remember, “Don’t play chess with Stephen King”). \n",
    "\n",
    "<center><table><tr>\n",
    "<td> <img src=\"https://github.com/jkropko/surfing-the-data-pipeline/blob/main/stephenking-sm.jpeg\" alt=\"Stephen King\" style=\"width: 150px;\"/> </td>\n",
    "<td> <img src=\"https://github.com/jkropko/surfing-the-data-pipeline/blob/main/Chess_board_opening_staunton.jpeg\" alt=\"Chess\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "<p style=\"text-align: center;\">Although acclaimed horror writer Stephen King would be a terrifying opponent, I can't find anything on the Internet that says he plays chess. <b>Sources:</b>\n",
    "<a href=\"https://stephenking.com/the-author/\">Author Stephen King's website\n",
    "</a> and <a href=\"https://en.wikipedia.org/wiki/Chessboard\">the Wikipedia page for a chessboard.\n",
    "</a>\n",
    "</p></center>\n",
    " \n",
    "* **Data**: Provide “just enough” data within the prompt to get the chatbot to write code that is more specific to the data at hand. That often means copy-and-pasting just a few rows in CSV or JSON format, and including it in the prompt while directly writing how you are providing data.\n",
    "* **Packages**: Directly ask for specific coding languages and packages if you know the right ones to use.\n",
    "* **Chunking**: Ask for elaboration and chunk the task. Chatbots like ChatGPT work in reference to previous queries, so it is appropriate to issue a second prompt that references the first to add complexity to the task.\n",
    "* **Streamline**: Chatbots are not guaranteed to generate efficient code in terms of length, even if the code works. Sometimes one line of code using a method in a package like `pandas` accomplishes what 50 lines of code can do in base python. Using the complicated version of the code makes your work harder for you and your collaborators to understand. You can issue a follow-up prompt to a chatbot to ask for code that accomplishes the same task with in a more streamlined way: “Provide code that works the same as the previous response but uses fewer lines.”\n",
    "* **Knowledge**: The main challenge, however, is knowing what to ask for and understanding whether or not the provided code genuinely accomplishes the task. If you aren’t specific with how you ask the question and don’t supply data, the chatbot will usually provide code that looks sort-of right but can’t be run to confirm whether it works as needed, and if you aren’t sure what the output ought to look like it will be impossible to know whether the code is correct or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Data from the World Bank\n",
    "Let's take as an example the task of wrangling data from the [World Bank's World Development Indicators (WDI)](https://databank.worldbank.org/source/world-development-indicators). The following data contain information from every country tracked by the World Bank on CO2 emissions, gross domestic product per capita, and population for every year from 2012 through 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Series Name</th>\n",
       "      <th>Series Code</th>\n",
       "      <th>2012 [YR2012]</th>\n",
       "      <th>2013 [YR2013]</th>\n",
       "      <th>2014 [YR2014]</th>\n",
       "      <th>2015 [YR2015]</th>\n",
       "      <th>2016 [YR2016]</th>\n",
       "      <th>2017 [YR2017]</th>\n",
       "      <th>2018 [YR2018]</th>\n",
       "      <th>2019 [YR2019]</th>\n",
       "      <th>2020 [YR2020]</th>\n",
       "      <th>2021 [YR2021]</th>\n",
       "      <th>2022 [YR2022]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>CO2 emissions (kt)</td>\n",
       "      <td>EN.ATM.CO2E.KT</td>\n",
       "      <td>10208.13</td>\n",
       "      <td>9402.05</td>\n",
       "      <td>9281.34</td>\n",
       "      <td>10057.59</td>\n",
       "      <td>9294.93</td>\n",
       "      <td>10022.78</td>\n",
       "      <td>10972.38</td>\n",
       "      <td>11238.83</td>\n",
       "      <td>8709.47</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>GDP per capita (constant 2015 US$)</td>\n",
       "      <td>NY.GDP.PCAP.KD</td>\n",
       "      <td>570.676064337005</td>\n",
       "      <td>582.103978902877</td>\n",
       "      <td>576.487820334782</td>\n",
       "      <td>566.881132665072</td>\n",
       "      <td>564.920843553397</td>\n",
       "      <td>563.488239421983</td>\n",
       "      <td>553.973308908649</td>\n",
       "      <td>559.140956898688</td>\n",
       "      <td>529.144912477888</td>\n",
       "      <td>407.616507361737</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>30466479</td>\n",
       "      <td>31541209</td>\n",
       "      <td>32716210</td>\n",
       "      <td>33753499</td>\n",
       "      <td>34636207</td>\n",
       "      <td>35643418</td>\n",
       "      <td>36686784</td>\n",
       "      <td>37769499</td>\n",
       "      <td>38972230</td>\n",
       "      <td>40099462</td>\n",
       "      <td>41128771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>CO2 emissions (kt)</td>\n",
       "      <td>EN.ATM.CO2E.KT</td>\n",
       "      <td>4541.8</td>\n",
       "      <td>4795.4</td>\n",
       "      <td>5188</td>\n",
       "      <td>4797</td>\n",
       "      <td>4573.2</td>\n",
       "      <td>5403.7</td>\n",
       "      <td>5316.1</td>\n",
       "      <td>4993.3</td>\n",
       "      <td>4383.2</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>GDP per capita (constant 2015 US$)</td>\n",
       "      <td>NY.GDP.PCAP.KD</td>\n",
       "      <td>3736.34007957648</td>\n",
       "      <td>3780.69919254096</td>\n",
       "      <td>3855.76074409659</td>\n",
       "      <td>3952.80357364813</td>\n",
       "      <td>4090.37272829183</td>\n",
       "      <td>4249.8200493985</td>\n",
       "      <td>4431.55559506989</td>\n",
       "      <td>4543.38771048312</td>\n",
       "      <td>4418.66087378292</td>\n",
       "      <td>4857.11194201819</td>\n",
       "      <td>5155.29085964151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>2900401</td>\n",
       "      <td>2895092</td>\n",
       "      <td>2889104</td>\n",
       "      <td>2880703</td>\n",
       "      <td>2876101</td>\n",
       "      <td>2873457</td>\n",
       "      <td>2866376</td>\n",
       "      <td>2854191</td>\n",
       "      <td>2837849</td>\n",
       "      <td>2811666</td>\n",
       "      <td>2777689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>CO2 emissions (kt)</td>\n",
       "      <td>EN.ATM.CO2E.KT</td>\n",
       "      <td>134934.2</td>\n",
       "      <td>139024.1</td>\n",
       "      <td>147735.2</td>\n",
       "      <td>156273</td>\n",
       "      <td>154654.3</td>\n",
       "      <td>157704.4</td>\n",
       "      <td>164534.1</td>\n",
       "      <td>170582.4</td>\n",
       "      <td>161563</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>GDP per capita (constant 2015 US$)</td>\n",
       "      <td>NY.GDP.PCAP.KD</td>\n",
       "      <td>4025.64148367191</td>\n",
       "      <td>4057.76480695524</td>\n",
       "      <td>4129.42254868204</td>\n",
       "      <td>4197.41998491398</td>\n",
       "      <td>4246.24217385308</td>\n",
       "      <td>4218.0823188856</td>\n",
       "      <td>4188.22038483483</td>\n",
       "      <td>4153.00345481624</td>\n",
       "      <td>3873.50874565769</td>\n",
       "      <td>3939.36086434716</td>\n",
       "      <td>3999.75769667754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>37260563</td>\n",
       "      <td>38000626</td>\n",
       "      <td>38760168</td>\n",
       "      <td>39543154</td>\n",
       "      <td>40339329</td>\n",
       "      <td>41136546</td>\n",
       "      <td>41927007</td>\n",
       "      <td>42705368</td>\n",
       "      <td>43451666</td>\n",
       "      <td>44177969</td>\n",
       "      <td>44903225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>ASM</td>\n",
       "      <td>CO2 emissions (kt)</td>\n",
       "      <td>EN.ATM.CO2E.KT</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country Name Country Code                         Series Name  \\\n",
       "0     Afghanistan          AFG                  CO2 emissions (kt)   \n",
       "1     Afghanistan          AFG  GDP per capita (constant 2015 US$)   \n",
       "2     Afghanistan          AFG                   Population, total   \n",
       "3         Albania          ALB                  CO2 emissions (kt)   \n",
       "4         Albania          ALB  GDP per capita (constant 2015 US$)   \n",
       "5         Albania          ALB                   Population, total   \n",
       "6         Algeria          DZA                  CO2 emissions (kt)   \n",
       "7         Algeria          DZA  GDP per capita (constant 2015 US$)   \n",
       "8         Algeria          DZA                   Population, total   \n",
       "9  American Samoa          ASM                  CO2 emissions (kt)   \n",
       "\n",
       "      Series Code     2012 [YR2012]     2013 [YR2013]     2014 [YR2014]  \\\n",
       "0  EN.ATM.CO2E.KT          10208.13           9402.05           9281.34   \n",
       "1  NY.GDP.PCAP.KD  570.676064337005  582.103978902877  576.487820334782   \n",
       "2     SP.POP.TOTL          30466479          31541209          32716210   \n",
       "3  EN.ATM.CO2E.KT            4541.8            4795.4              5188   \n",
       "4  NY.GDP.PCAP.KD  3736.34007957648  3780.69919254096  3855.76074409659   \n",
       "5     SP.POP.TOTL           2900401           2895092           2889104   \n",
       "6  EN.ATM.CO2E.KT          134934.2          139024.1          147735.2   \n",
       "7  NY.GDP.PCAP.KD  4025.64148367191  4057.76480695524  4129.42254868204   \n",
       "8     SP.POP.TOTL          37260563          38000626          38760168   \n",
       "9  EN.ATM.CO2E.KT                ..                ..                ..   \n",
       "\n",
       "      2015 [YR2015]     2016 [YR2016]     2017 [YR2017]     2018 [YR2018]  \\\n",
       "0          10057.59           9294.93          10022.78          10972.38   \n",
       "1  566.881132665072  564.920843553397  563.488239421983  553.973308908649   \n",
       "2          33753499          34636207          35643418          36686784   \n",
       "3              4797            4573.2            5403.7            5316.1   \n",
       "4  3952.80357364813  4090.37272829183   4249.8200493985  4431.55559506989   \n",
       "5           2880703           2876101           2873457           2866376   \n",
       "6            156273          154654.3          157704.4          164534.1   \n",
       "7  4197.41998491398  4246.24217385308   4218.0823188856  4188.22038483483   \n",
       "8          39543154          40339329          41136546          41927007   \n",
       "9                ..                ..                ..                ..   \n",
       "\n",
       "      2019 [YR2019]     2020 [YR2020]     2021 [YR2021]     2022 [YR2022]  \n",
       "0          11238.83           8709.47                ..                ..  \n",
       "1  559.140956898688  529.144912477888  407.616507361737                ..  \n",
       "2          37769499          38972230          40099462          41128771  \n",
       "3            4993.3            4383.2                ..                ..  \n",
       "4  4543.38771048312  4418.66087378292  4857.11194201819  5155.29085964151  \n",
       "5           2854191           2837849           2811666           2777689  \n",
       "6          170582.4            161563                ..                ..  \n",
       "7  4153.00345481624  3873.50874565769  3939.36086434716  3999.75769667754  \n",
       "8          42705368          43451666          44177969          44903225  \n",
       "9                ..                ..                ..                ..  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "worldbank = pd.read_csv('worldbank_data.csv')\n",
    "worldbank.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular way in which this dataset has been formatted by the World Bank places the three features in separate rows, and the years in separate columns. In order to conduct an analysis (regressing C02 emissions on GDP per capita and population, for example), I need years in the rows and the features in the columns. I also want to drop Series Code, to give the features in Series Name better code-friendly column names without parentheses or dollar signs, and I want to remove the bracketted labels such as [YR 2012] from the years. These tasks are just a few of the steps necessary to create \"clean data\" for a statistical model.\n",
    "\n",
    "I can use the `pandas` package to do this quickly. But for this example, I will use chatbot prompt engineering exclusively to try to accomplish the same result. I use the free ChatGPT website.\n",
    "\n",
    "First, I need to supply data. ChatGPT limits prompts to [4096 characters](https://www.howtogeek.com/895929/what-is-the-character-limit-for-chatgpt/), and that includes data included within the prompt. So the best practice is to narrow the data down to a small size that still illustrates the format of the data. Usually the first 5 rows of a dataframe, obtained with the `df.head()` method, is sufficient. The data still need to be pasted into the prompt in some text-based way, and comma-separated values (CSV) works well. A `pandas` dataframe can be converted to CSV-style text with the `df.to_csv(index=False)` method. To get a CSV version of the first 5 rows of the World Bank data, we can type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\\nAfghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\\nAfghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\\nAfghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\\nAlbania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\\nAlbania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldbank.head(5).to_csv(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the data and use it in the following prompt:\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using the following data in CSV format:\n",
    "```\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\\nAfghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\\nAfghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\\nAfghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\\nAlbania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\\nAlbania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\\n\n",
    "```\n",
    "Write code using python and pandas that represents the year columns as rows, represents the values of \"Series Name\" as columns, and drops \"Series Code\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Certainly! You can achieve this transformation using the pandas library in Python. Here's an example code:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = \"\"\"\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "\"\"\"\n",
    "\n",
    "# Read CSV data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "# Melt the DataFrame to transform year columns as rows\n",
    "df_melted = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Series Name'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Pivot the DataFrame to represent Series Name as columns\n",
    "df_pivoted = df_melted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "\n",
    "# Drop the 'Series Code' column\n",
    "df_pivoted = df_pivoted.drop(columns='Series Code')\n",
    "\n",
    "# Display the result\n",
    "print(df_pivoted)\n",
    "```\n",
    "\n",
    "This code reads the CSV data, melts the DataFrame to transform the year columns as rows, pivots the DataFrame to represent \"Series Name\" as columns, and finally drops the \"Series Code\" column.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the code supplied by the chatbot looks like it should work, I ran the code and it fails with an error that reads: \"TypeError: Could not convert string '..' to numeric\". Here I have to use my knowledge of the data. Further down the rows, some values use \"..\" as a missing value. The error indicates that these symbols are not recognized by python as a missing value and cause problems when converting columns to rows. So I issue a new prompt.\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Revise the previous code to replace all occurrences of \"..\" with missing values prior to melting or pivoting the dataframe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Certainly! You can use the `replace` function to replace all occurrences of \"..\" with missing values (NaN) before melting or pivoting the DataFrame. Here's the revised code:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = \"\"\"\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "\"\"\"\n",
    "\n",
    "# Read CSV data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "# Replace all occurrences of \"..\" with NaN\n",
    "df.replace(\"..\", pd.NA, inplace=True)\n",
    "\n",
    "# Melt the DataFrame to transform year columns as rows\n",
    "df_melted = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Series Name'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Pivot the DataFrame to represent Series Name as columns\n",
    "df_pivoted = df_melted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(df_pivoted)\n",
    "```\n",
    "\n",
    "This code first replaces all occurrences of \"..\" with NaN using the `replace` function before performing the melting and pivoting operations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code still produces an error that some values in the data cannot be converted to numeric format. So I try again by asking ChatGPT to convert all the columns that begin with a year to numeric after replacing the missing values.\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Revise the previous code to convert all columns that begin with \"20\" to numeric prior to melting or pivoting the dataframe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Certainly! You can convert all columns that begin with \"20\" to numeric before melting or pivoting the DataFrame. Here's the revised code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = '''\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "'''\n",
    "\n",
    "# Read CSV data into a DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "# Replace \"..\" with missing values\n",
    "df.replace('..', pd.NA, inplace=True)\n",
    "\n",
    "# Convert columns that begin with \"20\" to numeric\n",
    "df.iloc[:, 4:] = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivoted = df.melt(id_vars=['Country Name', 'Country Code', 'Series Name'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Drop 'Series Code' column\n",
    "df_pivoted = df_pivoted.drop(columns=['Series Name'])\n",
    "\n",
    "# Display the result\n",
    "df_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code includes a line to convert all columns that begin with \"20\" to numeric using `df.iloc[:, 4:] = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')` before pivoting the DataFrame.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code runs without error, but it does not generate the correct data. It drops \"Series Name\" instead of \"Series Code\", and it did not place the different values of \"Series Name\" in new columns. Let's see if we can get ChatGPT to fix these mistakes.\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alter the previous code by dropping \"Series Code\" instead of \"Series Name\", and by representing the values of \"Series Name\" in separate columns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Certainly! Here's the revised code to drop \"Series Code\" instead of \"Series Name\" and represent the values of \"Series Name\" in separate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = '''\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "'''\n",
    "\n",
    "# Read CSV data into a DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "# Replace \"..\" with missing values\n",
    "df.replace('..', pd.NA, inplace=True)\n",
    "\n",
    "# Convert columns that begin with \"20\" to numeric\n",
    "df.iloc[:, 4:] = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivoted = df.melt(id_vars=['Country Name', 'Country Code', 'Series Name', 'Series Code'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Drop 'Series Code' column\n",
    "df_pivoted = df_pivoted.drop(columns=['Series Code'])\n",
    "\n",
    "# Pivot the DataFrame again to have separate columns for each 'Series Name'\n",
    "df_pivoted = df_pivoted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "\n",
    "# Display the result\n",
    "df_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first melts the DataFrame and then pivots it again to have separate columns for each 'Series Name'. The 'Series Code' column is dropped before melting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data look very close to what we need. There are just a few more details to polish: I want to drop the labels like [YR 2012] next to each year, and I want to rename the columns. If I try to get ChatGPT to include these tasks in the prior code, it tends to forget to preserve the structure of the data we tried hard to build. A better approach would be to reset the conversation by passing a CSV-formatted string from the new data to ChatGPT.\n",
    "\n",
    "In short, the process of using the chatbot exclusively for data wrangling is slow and prone to error. But chatbots will improve in their coding ability in the future. Effective prompting is a useful skill worth practicing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering for Debugging or Explaining Existing Code\n",
    "Another use for chatbots in a data wrangling workflow is to debug or explain code. For debugging, the key is to isolate the code that generates the error and to supply the error message. That said, the chatbot's interpretation may be right or wrong for your specific case, the same as any post on Stack Overflow. The purpose of using a chatbot to work through errors is not to fix the code, exactly, but rather to shed some light on what is causing the error so that you can fix it yourself.\n",
    "\n",
    "For example, the chatbot's response to the first prompt we issued in the previous section yielded an error. I can pass the code and the important line in the error message to see what the chatbot says.\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following python code:\n",
    "```python\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = \"\"\"\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "\"\"\"\n",
    "\n",
    "# Read CSV data into a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "# Melt the DataFrame to transform year columns as rows\n",
    "df_melted = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Series Name'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Pivot the DataFrame to represent Series Name as columns\n",
    "df_pivoted = df_melted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "\n",
    "# Drop the 'Series Code' column\n",
    "df_pivoted = df_pivoted.drop(columns='Series Code')\n",
    "\n",
    "# Display the result\n",
    "print(df_pivoted)\n",
    "```\n",
    "yields an error \"TypeError: Could not convert string '..' to numeric\". What is the problem and how do we fix it?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The error you are encountering is due to the presence of '..' in your CSV data, which represents missing or incomplete data. When attempting to convert the data to numeric values, these '..' values cause a TypeError.\n",
    "\n",
    "To fix this issue, you can replace the '..' values with a suitable representation for missing data, such as `NaN` (Not a Number). You can modify the code to handle this during the reading of the CSV data by specifying the `na_values` parameter in the `read_csv` function:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = \"\"\"\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "\"\"\"\n",
    "\n",
    "# Read CSV data into a pandas DataFrame and replace '..' with NaN\n",
    "df = pd.read_csv(StringIO(csv_data), na_values='..')\n",
    "\n",
    "# Melt the DataFrame to transform year columns as rows\n",
    "df_melted = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Series Name'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Pivot the DataFrame to represent Series Name as columns\n",
    "df_pivoted = df_melted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "\n",
    "# Drop the 'Series Code' column\n",
    "df_pivoted = df_pivoted.drop(columns='Series Code')\n",
    "\n",
    "# Display the result\n",
    "print(df_pivoted)\n",
    "```\n",
    "\n",
    "This modification should resolve the TypeError by treating the '..' values as NaN during the conversion process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ChatGPT identified the issue and suggested some code that could fix it. It so happens that the code ChatGPT supplies still results in a (different) error, but at least one hurdle has been cleared.\n",
    "\n",
    "A second use for a chatbot is to provide an explanation for what happens line-by-line within a chunk of code. This ability is especially useful for a coder who already knows what they are doing, but needs some help tracing what someone else's code is doing. It can be a great way to learn new coding skills. For example, I can ask ChatGPT to explain the code that resulted in the best version of the data in the previous section.\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Please explain the following Python code line by line:\n",
    "```\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Input CSV data\n",
    "csv_data = '''\n",
    "Country Name,Country Code,Series Name,Series Code,2012 [YR2012],2013 [YR2013],2014 [YR2014],2015 [YR2015],2016 [YR2016],2017 [YR2017],2018 [YR2018],2019 [YR2019],2020 [YR2020],2021 [YR2021],2022 [YR2022]\n",
    "Afghanistan,AFG,CO2 emissions (kt),EN.ATM.CO2E.KT,10208.13,9402.05,9281.34,10057.59,9294.93,10022.78,10972.38,11238.83,8709.47,..,..\n",
    "Afghanistan,AFG,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,570.676064337005,582.103978902877,576.487820334782,566.881132665072,564.920843553397,563.488239421983,553.973308908649,559.140956898688,529.144912477888,407.616507361737,..\n",
    "Afghanistan,AFG,\"Population, total\",SP.POP.TOTL,30466479,31541209,32716210,33753499,34636207,35643418,36686784,37769499,38972230,40099462,41128771\n",
    "Albania,ALB,CO2 emissions (kt),EN.ATM.CO2E.KT,4541.8,4795.4,5188,4797,4573.2,5403.7,5316.1,4993.3,4383.2,..,..\n",
    "Albania,ALB,GDP per capita (constant 2015 US$),NY.GDP.PCAP.KD,3736.34007957648,3780.69919254096,3855.76074409659,3952.80357364813,4090.37272829183,4249.8200493985,4431.55559506989,4543.38771048312,4418.66087378292,4857.11194201819,5155.29085964151\n",
    "'''\n",
    "\n",
    "# Read CSV data into a DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "# Replace \"..\" with missing values\n",
    "df.replace('..', pd.NA, inplace=True)\n",
    "\n",
    "# Convert columns that begin with \"20\" to numeric\n",
    "df.iloc[:, 4:] = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivoted = df.melt(id_vars=['Country Name', 'Country Code', 'Series Name', 'Series Code'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Drop 'Series Code' column\n",
    "df_pivoted = df_pivoted.drop(columns=['Series Code'])\n",
    "\n",
    "# Pivot the DataFrame again to have separate columns for each 'Series Name'\n",
    "df_pivoted = df_pivoted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(df_pivoted)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Certainly! This Python code is using the pandas library to manipulate and transform a CSV-like dataset. Let's go through it line by line:\n",
    "\n",
    "1. **Import necessary libraries:**\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from io import StringIO\n",
    "   ```\n",
    "   - `pandas` is a powerful data manipulation library.\n",
    "   - `StringIO` from the `io` module is used to create an in-memory file-like object from a string.\n",
    "\n",
    "2. **Define CSV data as a string:**\n",
    "   ```python\n",
    "   csv_data = '''\n",
    "   ... (data here)\n",
    "   '''\n",
    "   ```\n",
    "   - The CSV data is a multiline string containing information about CO2 emissions, GDP per capita, and population for different countries over several years.\n",
    "\n",
    "3. **Read CSV data into a DataFrame:**\n",
    "   ```python\n",
    "   df = pd.read_csv(StringIO(csv_data))\n",
    "   ```\n",
    "   - `pd.read_csv` reads the CSV data from the provided string using `StringIO` and creates a DataFrame (`df`) from it.\n",
    "\n",
    "4. **Replace \"..\" with missing values:**\n",
    "   ```python\n",
    "   df.replace('..', pd.NA, inplace=True)\n",
    "   ```\n",
    "   - Replaces occurrences of \"..\" in the DataFrame with pandas' `pd.NA` (a representation of missing values) in-place.\n",
    "\n",
    "5. **Convert columns starting with \"20\" to numeric:**\n",
    "   ```python\n",
    "   df.iloc[:, 4:] = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
    "   ```\n",
    "   - Converts columns starting from the 5th column (index 4) to numeric values. The `errors='coerce'` parameter converts any non-numeric values to NaN.\n",
    "\n",
    "6. **Pivot the DataFrame:**\n",
    "   ```python\n",
    "   df_pivoted = df.melt(id_vars=['Country Name', 'Country Code', 'Series Name', 'Series Code'], var_name='Year', value_name='Value')\n",
    "   ```\n",
    "   - Melts the DataFrame to have a 'Country Name', 'Country Code', 'Series Name', 'Series Code', 'Year', and 'Value' columns. This is a common step for reshaping data.\n",
    "\n",
    "7. **Drop 'Series Code' column:**\n",
    "   ```python\n",
    "   df_pivoted = df_pivoted.drop(columns=['Series Code'])\n",
    "   ```\n",
    "   - Drops the 'Series Code' column as it's no longer needed.\n",
    "\n",
    "8. **Pivot the DataFrame again:**\n",
    "   ```python\n",
    "   df_pivoted = df_pivoted.pivot_table(index=['Country Name', 'Country Code', 'Year'], columns='Series Name', values='Value').reset_index()\n",
    "   ```\n",
    "   - Pivots the DataFrame again to have separate columns for each 'Series Name', creating a more structured format.\n",
    "\n",
    "9. **Display the result:**\n",
    "   ```python\n",
    "   print(df_pivoted)\n",
    "   ```\n",
    "   - Prints the final pivoted DataFrame.\n",
    "\n",
    "In summary, the code takes raw CSV-like data, cleans it, transforms it, and finally presents it in a more structured and readable format using pandas operations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6: Interacting with the Global Community of Python Users\n",
    "### Python Slack and Discord\n",
    "Slack and Discord are both platforms for groups of people to chat with each other, post to shared message boards for specific topics, share files, and hold meetings. \n",
    "\n",
    "The main slack page for the global community of Python users is **Pyslackers**: https://pyslackers.com/web. To join, just go to the URL and click \"join the community\". The main discussion happens on #python_, but there are many other useful channels including #data_science and #job_advice. This method is especially worthwhile if you already are used to using slack for work or for personal networking. \n",
    "\n",
    "**Python Discord** (https://www.pythondiscord.com/) is similar to PySlackers but seems to be more focused on organizing shared Python projects and posting the code on GitHub. To join, click the \"join us\" link in the upper-right corner of their homepage.\n",
    "\n",
    "One thing that helped me use Slack and Discord much more consistently is downloading the desktop app for Slack (https://slack.com/downloads) and for Discord (https://discord.com/download), and always leaving the apps open on my local computer so that I see messages right away when they are sent to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Chats With Python Users on Freenode\n",
    "The Python user community is world-wide, and for the most part, very supportive. There are active internet relay chat (IRC) networks where you can post a question to members who are also logged in, to possibly get an answer right away. The most active Python IRC is the #python channel on Freenode: https://webchat.freenode.net. When I logged in while writing this notebook, there were 1,778 people logged on.\n",
    "\n",
    "Internet chatrooms can be rough places, but the #python channel claims to enforce this [Code of Conduct](https://www.python.org/psf/codeofconduct/). Getting started on Freenode can be tricky, but it's easier if you follow these steps:\n",
    "\n",
    "1. Go to https://webchat.freenode.net/. Choose a nickname, and make it professional (you're a UVA student after all!) and unique. \n",
    "\n",
    "2. Don't write anything under channel. Prove you are not a robot by selecting pictures of motorcycles or something. Then, once your humanity has been established, click \"Start\".\n",
    "\n",
    "3. To use the #python channel, you need to register your nickname. To check if your nickname is unique, click on the \"freenode\" tab on the left-hand sidebar. A text box will appear on the bottom of the screen. Type:\n",
    "```\n",
    "/msg NickServ info\n",
    "```\n",
    "\n",
    "4. Step 3 will open a new tab. Switch to that tab. If no one else already has your nickname, you will see\n",
    "```\n",
    "NickServ: (notice) <nickname> is not registered.\n",
    "```\n",
    "If you see something else, it means someone already has your nickname. You can change your nickname right here by typing `/nick` followed by another nickname. Then type `/msg NickServ info` again. Repeat until you see the message listed above.\n",
    "\n",
    "**Important note**: DON'T use a password here that you use for important things like email, bank accounts, etc. We shouldn't have the same faith in the security of Freenode's servers as we can have in Google's. Also, this is the kind of platform that tends to attract hackers. And for people used to a graphical user interface, it might be easy to mistype in a way that accidentally displays your password in the chat. Use a unique, throwaway password!\n",
    "\n",
    "5. To register this nickname, type\n",
    "```\n",
    "/msg NickServ register <password> <email-address>\n",
    "```\n",
    "where `<password>` is a password you will use in the future, and `<email address>` is the email you want associated with this account.\n",
    "\n",
    "6. Check your email for a confirmation code. Be patient, it can take up to 20 minutes for the email to go through.\n",
    "\n",
    "7. Once you have the code, paste it and your nickname into this code, and submit it:\n",
    "```\n",
    "/msg NickServ VERIFY REGISTER <nickname> <secret-code>\n",
    "```\n",
    "\n",
    "8. You are now registered! Return to https://webchat.freenode.net/ and log-in with your nickname and password. Type #python under channel. You are free to chat away. Pay attention to the guidelines that appear as links on the top of the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Mailing Lists and Newsgroups\n",
    "[Usenet](https://en.wikipedia.org/wiki/Usenet) is a distributed discussion system (which means it has no central server). It was invented in 1979, and is still in use today. The Python Usenet message boards are at https://mail.python.org/archives/?sort=popular. The `comp.lang.python` board is for general discussions and questions about Python.\n",
    "\n",
    "The tutor mailing list (https://mail.python.org/mailman/listinfo/tutor) is for users who want to ask questions about learning computer programming with Python.\n",
    "\n",
    "If you have a question for the Python core development team, send an email to [help@python.org](mailto:help@python.org). The team is pretty busy, so be sure to check other resources and lists for an answer first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
